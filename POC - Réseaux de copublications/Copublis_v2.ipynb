{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebec4a81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================\n",
    "# EXTRACTION XML-TEI DE HAL : creation de fichiers xml qui seront utilis√©s pour la suite du script\n",
    "# =============================\n",
    "import os\n",
    "import time\n",
    "import requests\n",
    "import xml.etree.ElementTree as ET\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from tqdm import tqdm\n",
    "\n",
    "# =============================\n",
    "# PARAM√àTRES G√âN√âRAUX\n",
    "# =============================\n",
    "NOM_COLLECTION = \"INRIA\"\n",
    "ANNEES = list(range(2018, 2025))  # ann√©es √† traiter\n",
    "ROWS = 1000  # nombre de notices par requ√™te\n",
    "OUT_DIR = \"hal_xml_tei\"\n",
    "MAX_WORKERS = 4  # nombre de threads\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "BASE_URL = f\"https://api.archives-ouvertes.fr/search/{NOM_COLLECTION}\"\n",
    "\n",
    "# =============================\n",
    "# FONCTION PRINCIPALE PAR ANN√âE\n",
    "# =============================\n",
    "def harvest_year(year: int):\n",
    "    try:\n",
    "        print(f\"\\nüå± D√©marrage extraction HAL pour {year}\")\n",
    "\n",
    "        # --- r√©cup√©rer le nombre total de notices ---\n",
    "        count_params = {\n",
    "            \"q\": f\"publicationDateY_i:{year}\",\n",
    "            \"wt\": \"json\",\n",
    "            \"rows\": 0\n",
    "        }\n",
    "        r_count = requests.get(BASE_URL, params=count_params, timeout=60)\n",
    "        r_count.raise_for_status()\n",
    "        total = r_count.json()[\"response\"][\"numFound\"]\n",
    "\n",
    "        if total == 0:\n",
    "            print(f\"‚ö†Ô∏è  {year} : aucune notice\")\n",
    "            return\n",
    "\n",
    "        print(f\"üìö {year} : {total} notices HAL\")\n",
    "\n",
    "        cursor = \"*\"\n",
    "        page_num = 1\n",
    "\n",
    "        with tqdm(total=total, desc=f\"{year}\", position=year % 10) as pbar:\n",
    "            while True:\n",
    "                params = {\n",
    "                    \"q\": f\"publicationDateY_i:{year}\",\n",
    "                    \"wt\": \"xml-tei\",\n",
    "                    \"rows\": ROWS,\n",
    "                    \"sort\": \"docid asc\",\n",
    "                    \"cursorMark\": cursor\n",
    "                }\n",
    "\n",
    "                r = requests.get(BASE_URL, params=params, timeout=120)\n",
    "                r.raise_for_status()\n",
    "                text = r.text\n",
    "\n",
    "                # Nom du fichier par page\n",
    "                out_file_page = os.path.join(OUT_DIR, f\"HAL_{year}_page{page_num}.xml\")\n",
    "                with open(out_file_page, \"w\", encoding=\"utf-8\") as f_out:\n",
    "                    f_out.write(text)\n",
    "\n",
    "                # Lire next cursorMark dans le XML TEI\n",
    "                try:\n",
    "                    tree = ET.fromstring(text)\n",
    "                    next_cursor_mark = tree.attrib.get(\"next\")\n",
    "                except ET.ParseError:\n",
    "                    print(f\"‚ùå ERREUR parsing XML pour {year}, page {page_num}\")\n",
    "                    break\n",
    "\n",
    "                pbar.update(min(ROWS, total - pbar.n))\n",
    "                time.sleep(0.2)  # politesse HAL\n",
    "\n",
    "                if not next_cursor_mark or next_cursor_mark == cursor:\n",
    "                    break\n",
    "\n",
    "                cursor = next_cursor_mark\n",
    "                page_num += 1\n",
    "\n",
    "        print(f\"‚úÖ {year} termin√©, {page_num} pages extraites\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå ERREUR pour {year} : {e}\")\n",
    "\n",
    "# =============================\n",
    "# LANCEMENT PARALL√àLE\n",
    "# =============================\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"üöÄ Lancement extraction HAL (threads, par ann√©e)\\n\")\n",
    "\n",
    "    with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:\n",
    "        futures = {executor.submit(harvest_year, y): y for y in ANNEES}\n",
    "\n",
    "        for future in as_completed(futures):\n",
    "            year = futures[future]\n",
    "            try:\n",
    "                future.result()\n",
    "            except Exception as e:\n",
    "                print(f\"‚ùå Erreur pour {year} :\", e)\n",
    "\n",
    "    print(\"\\nüéâ Extraction HAL termin√©e\")\n",
    "\n",
    "# Comptage du nombre de notices r√©cup√©r√©es par ann√©es, pour contr√¥le\n",
    "\n",
    "\n",
    "import glob\n",
    "import os\n",
    "\n",
    "def count_biblfull_in_file(xml_file):\n",
    "    with open(xml_file, \"r\", encoding=\"utf-8\") as f:\n",
    "        text = f.read()\n",
    "    return text.count(\"<biblFull>\")\n",
    "\n",
    "grand_total = 0\n",
    "\n",
    "for year in range(2018, 2025):\n",
    "    # tous les fichiers pour cette ann√©e\n",
    "    files = sorted(glob.glob(os.path.join(\"hal_xml_tei\", f\"HAL_{year}_page*.xml\")))\n",
    "    total = 0\n",
    "    for xml_file in files:\n",
    "        n = count_biblfull_in_file(xml_file)\n",
    "        total += n\n",
    "    grand_total += total\n",
    "    print(f\"{year} : {total} notices (texte brut, toutes pages)\")\n",
    "\n",
    "print(f\"\\nüìö Total g√©n√©ral (2018-2024) : {grand_total} notices\")\n",
    "\n",
    "\n",
    "# Temps de traitement : 1mn 64sec.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bfe77b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#=============================================================================\n",
    "# exploitation des donn√©es des fichiers xml\n",
    "# Creation d'un df structure et d'un df auteurs √† partir de toutes les publications\n",
    "#=============================================================================\n",
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "# ==============================\n",
    "# PARAM√àTRES\n",
    "# ==============================\n",
    "XML_DIR = \"hal_xml_tei\"  # dossier contenant tous les fichiers XML\n",
    "tei_ns = \"http://www.tei-c.org/ns/1.0\"\n",
    "xml_ns = \"http://www.w3.org/XML/1998/namespace\"\n",
    "\n",
    "# ==============================\n",
    "# FONCTIONS\n",
    "# ==============================\n",
    "\n",
    "def extract_structures(text):\n",
    "    \"\"\"Extrait les structures d‚Äôun XML TEI (string)\"\"\"\n",
    "    listorg_match = re.search(\n",
    "        r'<listOrg[^>]*type=\"structures\">(.*?)</listOrg>',\n",
    "        text,\n",
    "        re.DOTALL\n",
    "    )\n",
    "    if not listorg_match:\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    listorg_text = listorg_match.group(1)\n",
    "    orgs_text = re.findall(r'(<org[^>]*>.*?</org>)', listorg_text, re.DOTALL)\n",
    "\n",
    "    structures = []\n",
    "    for org_block in orgs_text:\n",
    "        xml_id = re.search(r'xml:id=\"struct-(\\d+)\"', org_block)\n",
    "        valid_s = re.search(r'<org[^>]*status=\"([^\"]+)\"', org_block).group(1)\n",
    "        type_org = re.search(r'<org[^>]*type=\"([^\"]+)\"', org_block)\n",
    "        lenom = re.search(r'<orgName>([^<]+)</orgName>', org_block)\n",
    "        lacronyme = re.search(r'<orgName type=\"acronym\">([^<]+)</orgName>', org_block)\n",
    "        ladresse = re.findall(r'<addrLine>([^<]+)</addrLine>', org_block)\n",
    "        lepays = re.search(r'<country key=\"([^\"]+)\"', org_block)\n",
    "        lesrelations = re.findall(r'<relation[^>]*active=\"#struct-(\\d+)\"', org_block)\n",
    "        rnsr = re.search(r'<idno[^>]*type=\"RNSR\">([^<]+)</idno>', org_block)\n",
    "        ror_id = re.search(r'<idno[^>]*type=\"ROR\">([^<]+)</idno>', org_block)\n",
    "        structures.append({\n",
    "            \"id_Aurehal\": xml_id.group(1) if xml_id else \"\",\n",
    "            \"type\": type_org.group(1) if type_org else \"\",\n",
    "            \"statut\": valid_s if valid_s else \"\",\n",
    "            \"name\": lenom.group(1) if lenom else \"\",\n",
    "            \"acronym\": lacronyme.group(1) if lacronyme else \"\",\n",
    "            \"address\": \" \".join(ladresse) if ladresse else \"\",\n",
    "            \"country\": lepays.group(1) if lepays else \"\",\n",
    "            \"RNSR\": rnsr.group(1) if rnsr else \"\",\n",
    "            \"ror_id\": ror_id.group(1) if ror_id else \"\",\n",
    "            \"relations\": \",\".join(lesrelations)\n",
    "        })\n",
    "    return pd.DataFrame(structures)\n",
    "\n",
    "\n",
    "def extract_publis(tree, xml_file):\n",
    "    \"\"\"Extrait les publications et auteurs d‚Äôun XML TEI et ajoute le nom du fichier source\"\"\"\n",
    "    def T(tag): return f\"{{{tei_ns}}}{tag}\"\n",
    "    def get_full_text(elem):\n",
    "        return \"\".join(elem.itertext()).strip() if elem is not None else \"\"\n",
    "\n",
    "    records = []\n",
    "    root = tree.getroot()\n",
    "    biblfulls = root.findall(f\".//{T('body')}//{T('biblFull')}\")\n",
    "\n",
    "    for biblfull in biblfulls:\n",
    "        halID_elem = biblfull.find(f\".//{T('publicationStmt')}/{T('idno')}[@type='halId']\")\n",
    "        halID = halID_elem.text if halID_elem is not None else \"\"\n",
    "\n",
    "        date_elem = biblfull.find(f\".//{T('imprint')}/{T('date')}[@type='datePub']\")\n",
    "        date_prod = biblfull.find(f\".//{T('edition')}/{T('date')}[@type='whenProduced']\")\n",
    "        year = (date_elem.text if date_elem is not None else\n",
    "                date_prod.text if date_prod is not None else \"\")[:4]\n",
    "\n",
    "        keywords = biblfull.findall(f\".//{T('keywords')}/{T('term')}\")\n",
    "        keywords_str = \";\".join(\" \".join(k.text.split()) for k in keywords if k.text)\n",
    "\n",
    "        domains = biblfull.findall(f\".//{T('classCode')}[@scheme='halDomain']\")\n",
    "        hal_domain = \";\".join(d.text.strip() for d in domains if d.text)\n",
    "\n",
    "        # D√©clare le namespace TEI\n",
    "        namespaces = {'tei': 'http://www.tei-c.org/ns/1.0'}\n",
    "\n",
    "        # Type de doc\n",
    "        hal_typology_elem = biblfull.find(f\".//{T('classCode')}[@scheme='halTypology']\")\n",
    "        hal_typology_value = hal_typology_elem.get('n') if hal_typology_elem is not None else \"\"\n",
    "\n",
    "        abstract_elem = biblfull.find(f\".//{T('abstract')}[@{{{xml_ns}}}lang='en']\")\n",
    "        if abstract_elem is None:\n",
    "            abstract_elem = biblfull.find(f\".//{T('abstract')}[@{{{xml_ns}}}lang='fr']\")\n",
    "        abstract_str = get_full_text(abstract_elem)\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "        authors = biblfull.findall(f\".//{T('author')}\")\n",
    "        for author in authors or [None]:\n",
    "            if author is None:\n",
    "                author_name = \"Unknown\"\n",
    "                aff_ids = [\"\"]\n",
    "            else:\n",
    "                fn = author.find(f\".//{T('forename')}\")\n",
    "                sn = author.find(f\".//{T('surname')}\")\n",
    "                author_name = f\"{sn.text if sn is not None else 'Unknown'}, {fn.text if fn is not None else 'Unknown'}\"\n",
    "                affiliations = author.findall(f\".//{T('affiliation')}\")\n",
    "                aff_ids = [aff.get(\"ref\").lstrip(\"#struct-\") for aff in affiliations if aff.get(\"ref\")] or [\"\"]\n",
    "\n",
    "            for aff_id in aff_ids:\n",
    "                records.append({\n",
    "                    \"halID\": halID,\n",
    "                    \"year\": year,\n",
    "                    \"keywords\": keywords_str,\n",
    "                    \"hal_domain\": hal_domain,\n",
    "                    \"abstract\": abstract_str,\n",
    "                    \"author\": author_name,\n",
    "                    \"id_Aurehal\": aff_id,\n",
    "                    \"DocType\": hal_typology_value,\n",
    "                    \"xml_file\": xml_file  # colonne source\n",
    "                })\n",
    "\n",
    "    return pd.DataFrame(records)\n",
    "\n",
    "\n",
    "# ==============================\n",
    "# BOUCLE SUR TOUS LES FICHIERS DU DOSSIER\n",
    "# ==============================\n",
    "\n",
    "all_structures = []\n",
    "all_publis = []\n",
    "\n",
    "for file in os.listdir(XML_DIR):\n",
    "    if not file.endswith(\".xml\"):\n",
    "        continue\n",
    "    filepath = os.path.join(XML_DIR, file)\n",
    "    print(f\"Traitement {file} ...\")\n",
    "\n",
    "    with open(filepath, \"rb\") as f:  # lecture binaire pour g√©rer BOM\n",
    "        content = f.read()\n",
    "\n",
    "    # Supprimer BOM UTF-8 s‚Äôil existe\n",
    "    if content.startswith(b'\\xef\\xbb\\xbf'):\n",
    "        content = content[3:]\n",
    "\n",
    "    # D√©coder en utf-8 et enlever espaces / lignes vides au d√©but\n",
    "    text = content.decode(\"utf-8\").lstrip()\n",
    "\n",
    "    # structures\n",
    "    df_struct = extract_structures(text)\n",
    "    all_structures.append(df_struct)\n",
    "\n",
    "    # publications\n",
    "    tree = ET.ElementTree(ET.fromstring(text))\n",
    "    df_pub = extract_publis(tree, file)\n",
    "    all_publis.append(df_pub)\n",
    "\n",
    "# =====================================================================================================================================================\n",
    "# CONCAT√âNATION FINALE\n",
    "# CREATION DE DEUX LISTES DISTINCTES\n",
    "# - LISTE DES STRUCTURES D'AFFILIATIONS DES AUTEURS\n",
    "# - LISTE DES AUTEURS\n",
    "# =====================================================================================================================================================\n",
    "\n",
    "# Structures uniques par id_Aurehal\n",
    "df_structures = pd.concat(all_structures, ignore_index=True).drop_duplicates(subset=\"id_Aurehal\")\n",
    "\n",
    "# Publications cumul√©es\n",
    "df_auteurs = pd.concat(all_publis, ignore_index=True)\n",
    "\n",
    "# Supprimer les doublons exacts sur halID + author + id_Aurehal\n",
    "df_auteurs = df_auteurs.drop_duplicates(subset=[\"halID\", \"author\", \"id_Aurehal\"]).reset_index(drop=True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(f\"\\nNombre de structures uniques : {df_structures.shape[0]}\")\n",
    "print(f\"Nombre de publications (lignes auteur) : {df_auteurs.shape[0]}\")\n",
    "print(f\"Nombre de halID uniques : {df_auteurs['halID'].nunique()}\")\n",
    "\n",
    "# Temps de traitement pour toutes les ann√©es 2018-2025 : 30 sec. √† 1m10\n",
    "\n",
    "# df_auteurs.head(1)\n",
    "# df_structures.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a7b2757",
   "metadata": {},
   "outputs": [],
   "source": [
    "#========================================================\n",
    "# CROISEMENT STRUCTURES ET AUTEURS PAR L'ID AUREHAL\n",
    "#=========================================================\n",
    "\n",
    "df_auteurs_structures = \"\"\n",
    "# # Merge en gardant toutes les lignes de df_publis\n",
    "df_auteurs_structures = df_auteurs.merge(df_structures, on='id_Aurehal', how='left', suffixes=('', '_extra'))\n",
    "\n",
    "# Suppression de la colonne en double\n",
    "if 'id_Aurehal_extra' in df_auteurs_structures.columns:\n",
    "    df_auteurs_structures = df_auteurs_structures.drop(columns=['id_Aurehal_extra'])\n",
    "\n",
    "# Contr√¥les √©ventuels\n",
    "# df_auteurs_structures.head(1)\n",
    "# df_auteurs_structures[df_auteurs_structures[\"halID\"] == \"hal-01129393\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2c6cbca",
   "metadata": {},
   "outputs": [],
   "source": [
    "#==========================================================\n",
    "# IDENTIFIER LES AUTEURS INRIA ET AJOUTER LEUR CENTRE\n",
    "#==========================================================\n",
    "\n",
    "#==========================================================\n",
    "# Dictionnaire des centres et ID Aurehal\n",
    "#==========================================================\n",
    "\n",
    "# Dictionnaire id Aurehal ‚Üí centres\n",
    "codes_centres = {\n",
    "    \"419153\": \"Inria Univ. Rennes\",\n",
    "    \"104751\": \"Inria Univ. Bordeaux\",\n",
    "    \"34586\": \"Inria Univ. Cote Azur\",\n",
    "    \"2497\": \"Inria Univ. Grenoble\",\n",
    "    \"1096051\": \"Inria Lyon\",\n",
    "    \"129671\": \"Inria Univ. Lorraine\",\n",
    "    \"104752\": \"Inria Lille\",\n",
    "    \"118511\": \"Inria Saclay\",\n",
    "    \"454310\": \"Inria Paris\",\n",
    "    \"1175218\": \"Inria Paris Sorbonne\",\n",
    "    \"1225635\": \"Inria Saclay IPP\",\n",
    "    \"1225627\": \"Inria Saclay UPS\"\n",
    "}\n",
    "\n",
    "codes_set = set(codes_centres.keys())\n",
    "\n",
    "#==========================================================\n",
    "# Identifier les auteurs Inria\n",
    "#==========================================================\n",
    "def est_inria(value):\n",
    "    if pd.isna(value) or value == \"\":\n",
    "        return \"non\"\n",
    "    # S√©parer les codes (plusieurs codes peuvent √™tre s√©par√©s par \",\")\n",
    "    codes_in_value = [v.strip() for v in value.split(\",\")]\n",
    "    # V√©rifier si un code de centre Inria est pr√©sent\n",
    "    for code in codes_in_value:\n",
    "            if code in codes_centres:\n",
    "                return \"oui\"\n",
    "    return \"non\"\n",
    "\n",
    "# Appliquer sur id_Aurehal ou relations\n",
    "df_auteurs_structures[\"Inria\"] = df_auteurs_structures.apply(\n",
    "    lambda row: est_inria(row[\"id_Aurehal\"]) \n",
    "        if est_inria(row[\"id_Aurehal\"]) == \"oui\" \n",
    "        else est_inria(row[\"relations\"]),\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# Pour contr√¥le : ici, FOCUS, situ√© en Italie, doit √™tre tagg√© \"Inria=oui\"\n",
    "# df_auteurs_structures[df_auteurs_structures[\"halID\"] == \"hal-02378761\"] \n",
    "\n",
    "#==========================================================\n",
    "# AJOUT DU CENTRE INRIA\n",
    "#==========================================================\n",
    "\n",
    "def get_centre(value):\n",
    "    if pd.isna(value) or value == \"\":\n",
    "        return None\n",
    "    # S√©parer les codes (dans relations, plusieurs codes s√©par√©s par \",\")\n",
    "    codes_in_value = [v.strip() for v in value.split(\",\")]\n",
    "    # Chercher le premier code correspondant dans le dictionnaire\n",
    "    for code in codes_in_value:\n",
    "        if code in codes_centres:\n",
    "            return codes_centres[code]\n",
    "    return None\n",
    "\n",
    "# Appliquer sur id_Aurehal ou relations\n",
    "df_auteurs_structures[\"Centre\"] = df_auteurs_structures.apply(\n",
    "    lambda row: get_centre(row[\"id_Aurehal\"]) or get_centre(row[\"relations\"]),\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# Temps de traitement : env. 4 √† 24 sec\n",
    "# Contr√¥les √©ventuels\n",
    "# df_auteurs_structures[df_auteurs_structures[\"halID\"] == \"hal-01666389\"]\n",
    "# Flavio Oquendo doit √™tre en Inria = non\n",
    "# df_auteurs_structures[df_auteurs_structures[\"halID\"] == \"hal-01259762\"]\n",
    "# Boscain Ugo est √† la fois Inria et autre chose, on va supprimer l'autre\n",
    "# df_auteurs_structures[df_auteurs_structures[\"halID\"] == \"hal-01633660\"]\n",
    "\n",
    "# df_auteurs_structures[df_auteurs_structures[\"halID\"] == \"cel-01951107\"]\n",
    "# Khemakhem, Mohamed est Inria et aussi affiliation DE qui sera supprim√©e\n",
    "# Laurent Romary est Inria et aussi affiliation DE qui sera supprim√©e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59d66c2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#=========================================================\n",
    "# Suppression des auteurs non Inria ayant double affiliation FR et INT\n",
    "# Suppression des affiliations tierces des auteurs Inria\n",
    "# Suppression des auteurs Inria n'appartenant pas √† une √©quipe-projet\n",
    "#==========================================================================\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# √âtape 1 : Supprimer les lignes Inria=non pour les auteurs ayant des affiliations en France et √† l'√©tranger\n",
    "df_inria_non = df_auteurs_structures[df_auteurs_structures[\"Inria\"] == \"non\"]\n",
    "grouped_non = df_inria_non.groupby([\"halID\", \"author\"])[\"country\"].agg(set)\n",
    "auteurs_mixtes_non = set(\n",
    "    grouped_non[\n",
    "        grouped_non.apply(lambda countries: any(c in fr_codes for c in countries) and any(c not in fr_codes for c in countries))\n",
    "    ].index\n",
    ")\n",
    "\n",
    "mask_suppression_non_mixtes = (\n",
    "    df_auteurs_structures.set_index([\"halID\", \"author\"]).index.isin(auteurs_mixtes_non) &\n",
    "    (df_auteurs_structures[\"Inria\"] == \"non\")\n",
    ")\n",
    "\n",
    "df_auteurs_structures_uniqaff = df_auteurs_structures[~mask_suppression_non_mixtes].reset_index(drop=True)\n",
    "\n",
    "# √âtape 2 : Supprimer les lignes Inria=non pour les auteurs ayant √† la fois Inria=oui et Inria=non\n",
    "grouped_inria = df_auteurs_structures_uniqaff.groupby([\"halID\", \"author\"])[\"Inria\"].agg(set)\n",
    "auteurs_avec_inria_oui_et_non = set(\n",
    "    grouped_inria[\n",
    "        grouped_inria.apply(lambda x: \"oui\" in x and \"non\" in x)\n",
    "    ].index\n",
    ")\n",
    "\n",
    "mask_suppression_non_inria = (\n",
    "    df_auteurs_structures_uniqaff.set_index([\"halID\", \"author\"]).index.isin(auteurs_avec_inria_oui_et_non) &\n",
    "    (df_auteurs_structures_uniqaff[\"Inria\"] == \"non\")\n",
    ")\n",
    "\n",
    "df_auteurs_structures_uniqaff = df_auteurs_structures_uniqaff[~mask_suppression_non_inria].reset_index(drop=True)\n",
    "\n",
    "# √âtape 3 : Pour les auteurs Inria=oui, ne garder que les lignes o√π type=researchteam\n",
    "# √âtape 3 : Supprimer toutes les lignes o√π Inria=oui et type != researchteam\n",
    "mask_suppression_non_researchteam = (\n",
    "    (df_auteurs_structures_uniqaff[\"Inria\"] == \"oui\") &\n",
    "    (df_auteurs_structures_uniqaff[\"type\"] != \"researchteam\")\n",
    ")\n",
    "\n",
    "df_auteurs_inria_et_autres_pre = df_auteurs_structures_uniqaff[~mask_suppression_non_researchteam].reset_index(drop=True)\n",
    "\n",
    "# Temps de traitement : env. 30 sec.\n",
    "\n",
    "# Contr√¥le des modifications\n",
    "# df_auteurs_inria_et_autres_pre[df_auteurs_inria_et_autres_pre[\"halID\"] == \"hal-01666389\"]\n",
    "# Flavio Oquendo doit √™tre en Inria = non\n",
    "# Khalil Drira (co-auteur FR) sera supprim√© quand on s√©parera les auteurs Inria et les co-auteurs √©trangers\n",
    "# df_auteurs_inria_et_autres_pre[df_auteurs_inria_et_autres_pre[\"halID\"] == \"hal-01259762\"]\n",
    "# = Il devrait y avoir une seule ligne pour Boscain Ugo\n",
    "# df_auteurs_inria_et_autres_pre[df_auteurs_inria_et_autres_pre[\"halID\"] == \"cel-01951107\"]\n",
    "# Une seule ligne pour Khemakhem Mohamed et Romary Laurent\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "030bfd95",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_auteurs_inria_et_autres_pre.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27055812",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_auteurs_inria_et_autres_pre[\"halID\"].nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5fe510a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#==============================================================================================\n",
    "# SUPPRIMER TOUTES LES PUBLICATIONS QUI N'ONT QUE DES AUTEURS INRIA\n",
    "#==============================================================================================\n",
    "df_auteurs_structures_inria_et_autres_int = df_auteurs_inria_et_autres_pre.copy()\n",
    "\n",
    "print(f\"Nbre de publications avant : {df_auteurs_structures_inria_et_autres_int[\"halID\"].nunique()}\")\n",
    "df_auteurs_structures_inria_et_autres_final = \"\"\n",
    "\n",
    "\n",
    "# 1. Identifier les halID \"mixtes\" (au moins un \"oui\" et un \"non\" pour Inria)\n",
    "halID_mixtes = (\n",
    "    df_auteurs_structures_inria_et_autres_int\n",
    "    .groupby(\"halID\")[\"Inria\"]\n",
    "    .apply(lambda x: (\"oui\" in x.values) and (\"non\" in x.values))\n",
    ")\n",
    "halID_mixtes = halID_mixtes[halID_mixtes].index\n",
    "\n",
    "# 2. Filtrer le DataFrame pour ne garder que ces halID\n",
    "df_auteurs_structures_inria_et_autres_final = df_auteurs_structures_inria_et_autres_int[\n",
    "    df_auteurs_structures_inria_et_autres_int[\"halID\"].isin(halID_mixtes)\n",
    "].reset_index(drop=True)\n",
    "\n",
    "print(f\"Nbre de publications sans les publis uniquement Inria : {df_auteurs_structures_inria_et_autres_final[\"halID\"].nunique()}\")\n",
    "\n",
    "\n",
    "# Aucun enregistrement ne devrait correspondre (publication uniquement Inria)\n",
    "# df_auteurs_structures_inria_et_autres_final[df_auteurs_structures_inria_et_autres_final[\"halID\"] == \"tel-05202989\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84d3b421",
   "metadata": {},
   "outputs": [],
   "source": [
    "#==============================================================================================\n",
    "# CRER DEUX LISTES : AUTEURS INRIA ET AUTEURS INTERNATIONAUX\n",
    "#==============================================================================================\n",
    "import pandas as pd\n",
    "\n",
    "# Liste des codes pays fran√ßais √† exclure\n",
    "fr_codes = ['FR', 'GP', 'RE', 'MQ', 'GF', 'YT', 'PM', 'WF', 'TF', 'NC', 'PF']\n",
    "\n",
    "# 1. Cr√©er df_auteurs_inria : lignes o√π Inria = \"oui\"\n",
    "df_auteurs_inria = df_auteurs_structures_inria_et_autres_final[\n",
    "    df_auteurs_structures_inria_et_autres_final[\"Inria\"] == \"oui\"\n",
    "].reset_index(drop=True)\n",
    "\n",
    "# 2. Cr√©er df_auteurs_int : lignes o√π Inria = \"non\" ET country n'est pas dans fr_codes\n",
    "df_auteurs_int = df_auteurs_structures_inria_et_autres_final[\n",
    "    (df_auteurs_structures_inria_et_autres_final[\"Inria\"] == \"non\") &\n",
    "    (~df_auteurs_structures_inria_et_autres_final[\"country\"].isin(fr_codes))\n",
    "].reset_index(drop=True)\n",
    "\n",
    "# Contr√¥le √©ventuel : une seule ligne auteur\n",
    "# df_auteurs_int[df_auteurs_int[\"halID\"] == \"hal-01666389\"]\n",
    "\n",
    "# Nombre de copublications = nombre de Hal ID uniques dans les auteurs √©trangers\n",
    "# df_auteurs_int[\"halID\"].nunique()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c264ff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#================================================================================================\n",
    "# AJOUT DES DOMAINES ET MOTS-CLES INRIA AUX AUTEURS INRIA (acc√®s r√©serv√© au fichier source : Bastri)\n",
    "#=================================================================================================\n",
    "\n",
    "\n",
    "  # Import du fichier des √©quipes-projet comportant les domaines et mots-cl√©s\n",
    "#================================================================================================\n",
    "\n",
    "# \n",
    "df_bastri = pd.read_csv(\n",
    "    \"ExportBastri.csv\",\n",
    "    sep=';',           \n",
    "    dtype=str,\n",
    "    encoding='latin1', \n",
    "    on_bad_lines='skip'\n",
    ")\n",
    "\n",
    "\n",
    "# Nettoyage du fichier pour avoir une seule occurrence de chaque √©quipe-projet \n",
    "\n",
    "df_bastri_clean = (\n",
    "    df_bastri\n",
    "    .assign(has_domain=df_bastri[\"Domaine fran√ßais\"].notna())\n",
    "    .sort_values(\"has_domain\", ascending=False)\n",
    "    .drop_duplicates(subset=[\"Num. national\"], keep=\"first\")\n",
    "    .drop(columns=\"has_domain\")\n",
    ")\n",
    "\n",
    "  # Enrichissement de la liste des auteurs Inria\n",
    "#===========================================================================\n",
    "\n",
    "\n",
    "cols_to_get = [\"Domaine fran√ßais\", \"Mots cl√©s (fran√ßais)\"]\n",
    "\n",
    "df_auteurs_inria_bastri = \"\"\n",
    "# Merge : c√¥t√© bastri Num. national, c√¥t√© couples RNSR\n",
    "df_auteurs_inria_bastri = df_auteurs_inria.merge(\n",
    "    df_bastri_clean[[\"Num. national\"] + cols_to_get],\n",
    "    left_on=\"RNSR\",        # colonne dans df_couples\n",
    "    right_on=\"Num. national\",\n",
    "    how=\"left\",\n",
    "    suffixes=(\"\", \"_bastri\")\n",
    ")\n",
    "\n",
    "# df_auteurs_inria_bastri.head(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffd6a9fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#==========================================================================================================================================================================\n",
    "# POUR AUTEURS ETRANGERS :\n",
    "# LISTE DES AFFILIATIONS DE NIVEAU SUPERIEUR (INSTITUTION OU REGROUPINSTITUTION)\n",
    "#========================================================================================================================================================================\n",
    "df_auteurs_int_et_institutions = \"\"\n",
    "df_struct_institution_regroupinstit = \"\"\n",
    "\n",
    "df_struct_institution_regroupinstit = df_structures[\n",
    "    df_structures[\"type\"].isin([\"institution\", \"regroupinstitution\"])\n",
    "].copy()\n",
    "df_struct_institution_regroupinstit.head(2)\n",
    "\n",
    "\n",
    "\n",
    "# ====================================================================================================\n",
    "# AJOUT D'UNE AFFILIATION DE NIVEAU SUPERIEUR AUX AUTEURS INTERNATIONAUX\n",
    "# ====================================================================================================\n",
    "df_rel = \"\" \n",
    "df_expl = \"\" \n",
    "df_priority = \"\"\n",
    "# 1Ô∏è‚É£ Pr√©parer le mapping id_Aurehal -> type et les infos √† merger\n",
    "dict_type = df_struct_institution_regroupinstit.set_index(\"id_Aurehal\")[\"type\"].to_dict()\n",
    "df_infos = df_struct_institution_regroupinstit[[\"id_Aurehal\", \"name\", \"address\",\"type\", \"statut\", \"ror_id\"]].rename(\n",
    "    columns={\n",
    "        \"id_Aurehal\": \"Id_Aurehal_org_Top_copubliant\",\n",
    "        \"ror_id\":\"Ror_org_Top_copub\",\n",
    "        \"name\": \"Nom_org_Top_copubliant\",\n",
    "        \"address\": \"Adresse_org_Top_copubliant\",\n",
    "        \"type\":\"Type_org_Top_copubliant\",\n",
    "        \"statut\": \"Statut_org_Top_copubliant\",\n",
    "    }\n",
    ")\n",
    "\n",
    "# 2Ô∏è‚É£ Filtrer les lignes concern√©es (type != institution/regroupinstitution)\n",
    "mask = ~df_auteurs_int[\"type\"].isin([\"institution\", \"regroupinstitution\"])\n",
    "df_rel = df_auteurs_int.loc[mask, [\"relations\"]].copy()\n",
    "df_rel[\"orig_index\"] = df_rel.index\n",
    "\n",
    "# 3Ô∏è‚É£ Exploser les relations et ne garder que celles pr√©sentes dans df_struct_institution_regroupinstit\n",
    "df_rel[\"relations_list\"] = df_rel[\"relations\"].fillna(\"\").str.split(\",\")\n",
    "df_expl = df_rel.explode(\"relations_list\")\n",
    "df_expl[\"relations_list\"] = df_expl[\"relations_list\"].str.strip()\n",
    "df_expl = df_expl[df_expl[\"relations_list\"].isin(dict_type)]\n",
    "df_expl[\"type_relation\"] = df_expl[\"relations_list\"].map(dict_type)\n",
    "\n",
    "# 4Ô∏è‚É£ Choisir l'id_Aurehal prioritaire (regroupinstitution > institution)\n",
    "def choose_priority(g):\n",
    "    if \"regroupinstitution\" in g[\"type_relation\"].values:\n",
    "        return g.loc[g[\"type_relation\"]==\"regroupinstitution\", \"relations_list\"].iloc[0]\n",
    "    elif \"institution\" in g[\"type_relation\"].values:\n",
    "        return g.loc[g[\"type_relation\"]==\"institution\", \"relations_list\"].iloc[0]\n",
    "    return None\n",
    "\n",
    "# Priorit√© regroupinstitution > institution\n",
    "df_expl[\"priority\"] = df_expl[\"type_relation\"].map({\"regroupinstitution\": 1, \"institution\": 2})\n",
    "df_priority = df_expl.sort_values(\"priority\").groupby(\"orig_index\")[\"relations_list\"].first().rename(\"Id_Aurehal_org_Top_copubliant\")\n",
    "\n",
    "#==================================\n",
    "# ASSOCIER LES INSTITUTIONS DE NIVEAU SUPERIEUR AUX CO-AUTEURS INTERNATIONAUX\n",
    "#+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
    "\n",
    "# 5Ô∏è‚É£ R√©associer au df original\n",
    "df_auteurs_int_et_institutions = df_auteurs_int.join(df_priority, how=\"left\")\n",
    "\n",
    "# 6Ô∏è‚É£ Ajouter nom et adresse de l'institution/ regroupinstitution\n",
    "df_auteurs_int_et_institutions = df_auteurs_int_et_institutions.merge(\n",
    "    df_infos,\n",
    "    on=\"Id_Aurehal_org_Top_copubliant\",\n",
    "    how=\"left\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a94bc8e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#===========================================================================\n",
    "# Regroupement auteurs Inria et auteurs √©trangers par publication\n",
    "#============================================================================\n",
    "df_inria_r = \"\"\n",
    "df_int_r = \"\"\n",
    "df_copublis_final = \"\"\n",
    "# renommage de certaines colonnes\n",
    "df_inria_r = df_auteurs_inria_bastri.rename(columns={\"author\": \"Auteur_inria\"})\n",
    "df_int_r = df_auteurs_int_et_institutions.rename(columns={\"author\": \"Auteur_international\"})\n",
    "\n",
    "df_copublis_final = df_inria_r.merge(\n",
    "    df_int_r,\n",
    "    on=\"halID\",\n",
    "    how=\"inner\",\n",
    "    suffixes=(\"_halinria\", \"_int\")\n",
    ")\n",
    "\n",
    "\n",
    "# Contr√¥les\n",
    "# df_copublis_final[df_copublis_final[\"halID\"] == \"hal-01633660\"][\n",
    "#     [\"Auteur_inria\", \"Auteur_international\",\"ror_id_int\"]]\n",
    "# r√©sultat attendu : Maugey, Thomas - Ma, Rui\n",
    " #                   Frossard, Pascal - Ma, Rui\n",
    "\n",
    "\n",
    "\n",
    "# df_copublis_final[df_copublis_final[\"halID\"] == \"cel-01951107\"][\n",
    "#     [\"Auteur_inria\", \"Auteur_international\",\"ror_id_int\"]]\n",
    "# R√©sultat attendu : \n",
    "# Khemakhem, Mohamed - Gabay, Simon\n",
    "# Romary, Laurent - Gabay, Simon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff3541ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "#=======================================================\n",
    "# Nettoyage apr√®s fusion\n",
    "#=======================================================\n",
    "# Nettoyage  (on supprime tous les espaces avant et apr√®s)\n",
    "df_copublis_final = df_copublis_final.apply(\n",
    "    lambda col: col.str.strip() if col.dtype == \"object\" else col\n",
    ")\n",
    "\n",
    "# Supprimer la colonne Num. national \n",
    "df_copublis_final = df_copublis_final.drop(columns=[\"Num. national\"])\n",
    "\n",
    "# Renommer les colonnes Domaine et mots-cl√©s\n",
    "df_copublis_final = df_copublis_final.rename(columns={\n",
    "    \"Domaine fran√ßais\": \"Domaine_inria\",\n",
    "    \"Mots cl√©s (fran√ßais)\": \"Mots_cles_inria\"\n",
    "})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7810523d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#=============================================================================\n",
    "# Contr√¥le : Lignes o√π acronym est renseign√© mais Domaine_inria est vide\n",
    "# En principe sont vide : SED, affiliations directes √† un Centre\n",
    "#==============================================================================\n",
    "df_sans_domaine = \"\"\n",
    "df_sans_domaine = df_copublis_final[\n",
    "    df_copublis_final[\"acronym_halinria\"].notna() & df_copublis_final[\"Domaine_inria\"].isna()\n",
    "]\n",
    "\n",
    "\n",
    "# Liste des valeurs pour lesquelles on n'a pas trouv√© de domaine ou de mots-cl√©s dans Bastri\n",
    "\n",
    "df_grouped = (\n",
    "    df_sans_domaine\n",
    "    .groupby(\"acronym_halinria\", as_index=False)\n",
    "    .agg({\n",
    "        \"RNSR_halinria\": lambda x: \", \".join(x.dropna().unique()),\n",
    "        \"Domaine_inria\": lambda x: \", \".join(x.dropna().unique()),\n",
    "        \"halID\": lambda x: \", \".join(x.dropna().unique())\n",
    "    })\n",
    ")\n",
    "\n",
    "df_grouped\n",
    "df_grouped.to_excel(\"equipes_sans_domaines.xlsx\", index=False)\n",
    "\n",
    "\n",
    "\n",
    "# Temps de traitement env 10 sec."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c07536a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#===========================================\n",
    "# Ajout d'une colonne \"UE/hors UE\"\n",
    "#===========================================\n",
    "\n",
    "# Liste des codes pays UE ISO alpha-2\n",
    "ue_codes = [\n",
    "    \"AT\", \"BE\", \"BG\", \"HR\", \"CY\", \"CZ\", \"DK\", \"EE\", \"FI\", \"FR\",\n",
    "    \"DE\", \"GR\", \"HU\", \"IE\", \"IT\", \"LV\", \"LT\", \"LU\", \"MT\", \"NL\",\n",
    "    \"PL\", \"PT\", \"RO\", \"SK\", \"SI\", \"ES\", \"SE\"\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "# Ajouter la colonne \"UE/Hors UE\"\n",
    "df_copublis_final[\"UE/Hors_UE\"] = df_copublis_final[\"country_int\"].apply(\n",
    "    lambda x: \"UE\" if x in ue_codes else \"Hors_UE\"\n",
    ")\n",
    "print(\"UE/hors UE termin√©\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abf1e594",
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################################################\n",
    "# Interpr√©tation des codes Pays en noms en toutes lettres \n",
    "###########################################################\n",
    "\n",
    "# R√©cup√©rer les donn√©es de l'API\n",
    "# url = \"https://restcountries.com/v3.1/all\"\n",
    "# response = requests.get(url)\n",
    "# countries_data = response.json()\n",
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "# --- Fonction pour r√©cup√©rer le mapping code ‚Üí nom ---\n",
    "def get_country_mapping():\n",
    "    url = \"https://restcountries.com/v3.1/all\"\n",
    "    params = {\"fields\": \"cca2,name\"}\n",
    "    \n",
    "    try:\n",
    "        response = requests.get(url, params=params, timeout=10)\n",
    "        response.raise_for_status()  # l√®ve une erreur si le code HTTP n'est pas 200\n",
    "        countries_data = response.json()\n",
    "        \n",
    "        if isinstance(countries_data, list):\n",
    "            return {\n",
    "                country.get(\"cca2\"): country.get(\"name\", {}).get(\"common\")\n",
    "                for country in countries_data\n",
    "                if country.get(\"cca2\") and \"name\" in country and \"common\" in country[\"name\"]\n",
    "            }\n",
    "        else:\n",
    "            print(\"‚ö†Ô∏è Format inattendu :\", type(countries_data))\n",
    "            return {}\n",
    "    except Exception as e:\n",
    "        print(\"‚ùå Erreur lors de la r√©cup√©ration des donn√©es pays :\", e)\n",
    "        return {}\n",
    "\n",
    "# --- R√©cup√©ration du mapping ---\n",
    "country_mapping = get_country_mapping()\n",
    "print(\"‚úÖ Exemple : FR ‚Üí\", country_mapping.get(\"FR\"))\n",
    "\n",
    "\n",
    "\n",
    "# --- Ajout de la colonne avec le nom complet du pays ---\n",
    "df_copublis_final[\"Nom_Pays_org_copubliant\"] = df_copublis_final[\"country_int\"].map(\n",
    "    lambda code: country_mapping.get(code, code)  # garde le code si non trouv√©\n",
    ")\n",
    "\n",
    "# --- Affichage pour v√©rifier ---\n",
    "# print(df_couples_bastri)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "163a9ddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_copublis_final.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e93ed22",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#=========================================================================================\n",
    "# On renomme et on ordonne les colonnes, on supprime les colonnes inutiles ou redondantes\n",
    "#=========================================================================================\n",
    "\n",
    "# dictionnaire de renommage\n",
    "rename_cols = {\n",
    "    \"halID\" : \"Hal_ID\",\n",
    "    \"Type_doc\":\"DocType_halinria\",\n",
    "    \"year_halinria\": \"Annee\",\n",
    "    \"Centre_halinria\": \"Centre_inria\",\n",
    "    \"acronym_halinria\": \"Equipe_inria\",\n",
    "    \"Auteur_inria\": \"Auteur_Inria\",\n",
    "    \"Auteur_international\": \"Auteur_etranger\",\n",
    "    \"name_int\": \"Nom_org_copubliant\",\n",
    "    \"address_int\": \"Adresse_org_copubliant\",\n",
    "    \"type_int\": \"Type_org_copubliant\",\n",
    "    \"Nom_org_Top_copubliant\": \"Nom_org_Top_copubliant\",\n",
    "    \"Adresse_org_Top_copubliant\": \"Adresse_org_Top_copubliant\",\n",
    "    \"Type_org_Top_copubliant\": \"Type_org_Top_copubliant\",\n",
    "    \"country_int\": \"Code_Pays_orgs_copubliant\",\n",
    "    \"Nom_Pays_org_copubliant\" : \"Nom_Pays_org_copubliant\",\n",
    "    \"UE/Hors_UE\": \"UE/Hors_UE\",\n",
    "    \"abstract_halinria\": \"Resume\",\n",
    "    \"hal_domain_halinria\": \"Domaine_Hal\",\n",
    "    \"keywords_halinria\": \"Mots_cles_Hal\",\n",
    "    \"Domaine_inria\": \"Domaine_inria\",\n",
    "    \"Mots_cles_inria\": \"Mots_cles_inria\",\n",
    "    \"id_Aurehal_halinria\": \"Id_Aurehal_Equipe\",\n",
    "    \"ror_id_int\": \"Ror_org_copubliant\",\n",
    "    \"type_halinria\": \"Type_org_inria\",\n",
    "    \"name_halinria\": \"Nom_equipe_inria\",\n",
    "    \"address_halinria\": \"Adresse_equipe_inria\",\n",
    "    \"RNSR_halinria\": \"RNSR_Equipe\",\n",
    "    \"id_Aurehal_int\": \"id_Aurehal_org_copubliant\",\n",
    "    \"statut_halinria\": \"statut_org_copubliant\",\n",
    "    \"Ror_org_Top_copub\" : \"Ror_org_Top_copubliant\",\n",
    "    \"Id_Aurehal_org_Top_copubliant\": \"id_Aurehal_org_Top_copubliant\",\n",
    "    \"Statut_org_Top_copubliant\": \"Statut_org_Top_copubliant\",\n",
    "    \"relations_int\": \"Autres_affiliations_copubliant\",\n",
    "    \"xml_file_halinria\": \"Source\"\n",
    "}\n",
    "\n",
    "# appliquer le renommage\n",
    "df_copublis_final = df_copublis_final.rename(columns=rename_cols)\n",
    "\n",
    "# r√©ordonner les colonnes selon l'ordre voulu\n",
    "df_copublis_final = df_copublis_final[list(rename_cols.values())]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Suprression des colonnes inutiles\n",
    "cols_to_drop = [\n",
    "    \"country_halinria\",\n",
    "    \"relations_halinria\",\n",
    "    \"year_int\",\n",
    "    \"keywords_int\",\n",
    "    \"hal_domain_int\",\n",
    "    \"abstract_int\",\n",
    "    \"xml_file_int\",\n",
    "    \"acronym_int\",\n",
    "    \"RNSR_int\",\n",
    "    \"Centre_int\"\n",
    "]\n",
    "\n",
    "df_copublis_final = df_copublis_final.drop(columns=[c for c in cols_to_drop if c in df_copublis_final.columns])\n",
    "\n",
    "print(\"Renommage colonnes termin√©\")\n",
    "# df_copublis_final.head(1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "213b98a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#========================================\n",
    "# Suppression de toutes les lignes o√π le co-auteur n'a pas d'affiliation\n",
    "#==================================================\n",
    "print(len(df_copublis_final))\n",
    "df_copublis_final = df_copublis_final[df_copublis_final[\"Nom_org_copubliant\"].notna()]\n",
    "\n",
    "print(len(df_copublis_final))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6261bb2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#========================================\n",
    "# Pour les org sans org TOP (tutelle), mais de type \"institution\", on recopie le nom et l'adresse\n",
    "# dans les colonne des tutelles (Top)\n",
    "#==================================================\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "df_copublis_final = df_copublis_final.copy()\n",
    "\n",
    "# Nettoyer les colonnes ROR pour ne garder que l'identifiant\n",
    "df_copublis_final['Ror_org_Top_copubliant'] = df_copublis_final['Ror_org_Top_copubliant'].str.replace(r'https?://ror\\.org/', '', regex=True)\n",
    "df_copublis_final['Ror_org_copubliant'] = df_copublis_final['Ror_org_copubliant'].str.replace(r'https?://ror\\.org/', '', regex=True)\n",
    "\n",
    "\n",
    "# 1. Remplir 'Nom_org_Top_copubliant' et 'Adresse_org_Top_copubliant' si 'Type_org_copubliant' est 'institution' ou 'regroupinstitution'\n",
    "mask = (df_copublis_final['Type_org_copubliant'].isin(['institution', 'regroupinstitution'])) & (df_copublis_final['Nom_org_Top_copubliant'].isna())\n",
    "df_copublis_final.loc[mask, 'Nom_org_Top_copubliant'] = df_copublis_final['Nom_org_copubliant']\n",
    "df_copublis_final.loc[mask, 'Adresse_org_Top_copubliant'] = df_copublis_final['Adresse_org_copubliant']\n",
    "df_copublis_final.loc[mask, 'Ror_org_Top_copubliant'] = df_copublis_final['Ror_org_copubliant']\n",
    "\n",
    "# 2. Remplir l'adresse de org_Top avec celle de org_copubliant quand elle manque\n",
    "mask2 = (df_copublis_final['Nom_org_copubliant'].notna()) & (df_copublis_final['Nom_org_Top_copubliant'].isna())\n",
    "df_copublis_final.loc[mask2, 'Adresse_org_Top_copubliant'] = df_copublis_final['Adresse_org_copubliant']\n",
    "\n",
    "# 3. Si le niveau Top n'a pas de ROR, on reprend celui de org\n",
    "mask3 = (df_copublis_final['Ror_org_Top_copubliant'].isna())\n",
    "df_copublis_final.loc[mask3, 'Ror_org_Top_copubliant'] = df_copublis_final['Ror_org_copubliant']\n",
    "\n",
    "\n",
    "# 3. Cr√©er un DataFrame pour les autres valeurs de 'Type_org_copubliant'\n",
    "other_types = ((df_copublis_final['Nom_org_Top_copubliant'].isna()))\n",
    "            \n",
    "other_df = df_copublis_final[other_types].copy()\n",
    "\n",
    "\n",
    "# Grouper par 'Nom_org_Top_copubliant' et concat√©ner les 'Hal_ID'\n",
    "grouped = other_df.groupby('Nom_org_copubliant').agg({\n",
    "    'Adresse_org_copubliant': 'first',\n",
    "    'Type_org_copubliant': 'first',\n",
    "    'id_Aurehal_org_copubliant':'first',\n",
    "    'Hal_ID': lambda x: ', '.join(map(str, x)),\n",
    "}).reset_index()\n",
    "\n",
    "# Afficher les r√©sultats\n",
    "\n",
    "print(\"Recopie de org vers org TOP fait l√† o√π c'√©tait n√©cessaire\")\n",
    "\n",
    "# Cr√©ation du xlsx\n",
    "df_copublis_final['Ror_org_Top_copubliant'] = df_copublis_final['Ror_org_Top_copubliant'].astype(str)\n",
    "df_copublis_final['Ror_org_copubliant'] = df_copublis_final['Ror_org_copubliant'].astype(str)\n",
    "\n",
    "\n",
    "# df_copublis_final.to_excel(\"copublications_Inria_2018-2024_sans_villes_a_nettoyer.xlsx\", index=False)\n",
    "\n",
    "# print(\"Fichier Excel 'copublications_Inria_2018-2024_sans_villes' g√©n√©r√©\")\n",
    "\n",
    "print(\"G√©n√©ration du fichier des institutions √©trang√®res sans tutelles dans hal (Orgs_sans_Top)\")\n",
    "grouped.to_excel(\"orgs_sans_Top.xlsx\", index=False)\n",
    "\n",
    "# temps de traitement : 1 √† 3m "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "756676a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#=============================================================================\n",
    "# Suppression des variantes du titre autres qu'anglaises \n",
    "#===================================================================\n",
    "\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# Copie du DataFrame original\n",
    "df_couples_adresses_eng = df_copublis_final.copy(deep=True)\n",
    "\n",
    "def extraire_version_anglaise(nom):\n",
    "    if pd.isna(nom):\n",
    "        return nom\n",
    "\n",
    "    # V√©rifier si la valeur contient un \"=\"\n",
    "    if \"=\" not in nom:\n",
    "        return nom\n",
    "\n",
    "    # Mots-cl√©s typiques des noms d'institutions en anglais\n",
    "    mots_cles_anglais = [\n",
    "        r'University', r'Institute', r'Center', r'of ', r'for ',\n",
    "        r'Research', r'Lab', r'Department', r'School', r'College'\n",
    "    ]\n",
    "\n",
    "    # Convertir en cha√Æne de caract√®res\n",
    "    nom = str(nom)\n",
    "\n",
    "    # Chercher une sous-cha√Æne contenant un des mots-cl√©s anglais\n",
    "    for mot in mots_cles_anglais:\n",
    "        pattern = re.compile(r'([^=]*{}[^=]*)'.format(mot), re.IGNORECASE)\n",
    "        match = pattern.search(nom)\n",
    "        if match:\n",
    "            return match.group().strip()\n",
    "\n",
    "    # Si aucun mot-cl√© trouv√©, v√©rifier si une partie est en caract√®res latins\n",
    "    parties = [p.strip() for p in nom.split('=')]\n",
    "    for partie in parties:\n",
    "        if re.search(r'^[A-Za-z0-9\\s\\[\\]\\,\\.\\-]+$', partie):\n",
    "            return partie\n",
    "\n",
    "    # Sinon, prendre la derni√®re partie apr√®s le dernier \"=\"\n",
    "    return parties[-1] if parties else nom\n",
    "\n",
    "# Appliquer la fonction uniquement aux valeurs contenant un \"=\"\n",
    "mask = df_couples_adresses_eng['Nom_org_copubliant'].str.contains('=', na=False)\n",
    "df_couples_adresses_eng.loc[mask, 'Nom_org_copubliant'] = df_couples_adresses_eng.loc[mask, 'Nom_org_copubliant'].apply(extraire_version_anglaise)\n",
    "\n",
    "# Afficher le r√©sultat\n",
    "# print(df_couples_adresses_eng['Nom_org_copubliant'].head(10))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1480179",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Supprimer toutes les valeurs Nan (pour le Dashboard)\n",
    "df_couples_adresses_eng = df_couples_adresses_eng.fillna(\"\")\n",
    "\n",
    "# Enregistrement sous forme de fichier Excel\n",
    "df_copublis_final_eng = df_couples_adresses_eng.copy()\n",
    "\n",
    "df_copublis_final_eng.to_excel(\"copublications_Inria_2018-2024_sans_villes.xlsx\", index=False)\n",
    "\n",
    "# Temps de traitement : 2 √† 4 mn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a062bf42",
   "metadata": {},
   "outputs": [],
   "source": [
    "#================================\n",
    "# Suppression de tous les caract√®res invisibles\n",
    "#======================================\n",
    "\n",
    "\n",
    "def remove_invisible_chars(text):\n",
    "    if not isinstance(text, str):\n",
    "        return text\n",
    "    return \"\".join(\n",
    "        c for c in text\n",
    "        if not unicodedata.category(c).startswith(\"C\")\n",
    "    )\n",
    "\n",
    "# Appliquer √† toutes les colonnes texte\n",
    "df_couples_adresses = df_couples_adresses_eng.copy()\n",
    "for col in df_couples_adresses.select_dtypes(include=\"object\").columns:\n",
    "    df_couples_adresses[col] = df_couples_adresses[col].apply(remove_invisible_chars)\n",
    "\n",
    "# Temps de traitement : 40 secondes √† 3 mn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85f6e16f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#================================\n",
    "# Normalisation des apostrophes, guillemets et tirets typographiques\n",
    "#======================================\n",
    "TYPO_MAP = {\n",
    "    # Apostrophes\n",
    "    \"\\u2019\": \"'\",\n",
    "    \"\\u2018\": \"'\",\n",
    "    \"\\u02BC\": \"'\",  # modifier letter apostrophe\n",
    "    \"\\u02BB\": \"'\",  # turned comma / okina\n",
    "    # Guillemets\n",
    "    \"\\u201C\": '\"',\n",
    "    \"\\u201D\": '\"',\n",
    "    # Tirets\n",
    "    \"\\u2010\": \"-\",  # hyphen\n",
    "    \"\\u2011\": \"-\",  # non-breaking hyphen\n",
    "    \"\\u2012\": \"-\",  # figure dash\n",
    "    \"\\u2013\": \"-\",  # en dash\n",
    "    \"\\u2014\": \"-\",  # em dash\n",
    "    \"\\u2212\": \"-\",  # minus sign\n",
    "    # Virgules\n",
    "    \"\\u060C\": \",\",  # Arabic comma ‚Üí ASCII comma\n",
    "    \"\\u2019\": \"'\",  # apostrophe typographique\n",
    "    \"\\u2018\": \"'\",\n",
    "    \"\\u2013\": \"-\",  # en dash\n",
    "    \"\\u2014\": \"-\",  # em dash\n",
    "}\n",
    "\n",
    "def normalize_typography(text):\n",
    "    if not isinstance(text, str):\n",
    "        return text\n",
    "    for k, v in TYPO_MAP.items():\n",
    "        text = text.replace(k, v)\n",
    "    return text\n",
    "\n",
    "for col in df_couples_adresses.select_dtypes(include=\"object\").columns:\n",
    "    df_couples_adresses[col] = df_couples_adresses[col].apply(normalize_typography)\n",
    "\n",
    "    # Temps de traitement 17 sec\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea0c9ad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Supprimer toutes les valeurs Nan (pour le Dashboard)\n",
    "df_couples_adresses_eng = df_couples_adresses.fillna(\"\")\n",
    "\n",
    "# Enregistrement sous forme de fichier Excel\n",
    "df_copublis_final_eng = df_couples_adresses_eng.copy()\n",
    "\n",
    "df_copublis_final_eng.to_excel(\"copublications_Inria_2018-2024_sans_villes.xlsx\", index=False)\n",
    "\n",
    "# Temps de traitement : 2 √† 4 mn"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
