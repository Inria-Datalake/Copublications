{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2a540231",
   "metadata": {},
   "source": [
    "# Collection: copublications internationales (UE et hors UE)\n",
    "* demande interne INRIA (27/07/2025)\n",
    "* Réalisation du script (adaptation d'un ancien script) : Kumar Guha (Data DCIS/Inria)\n",
    "* Date 27/02/2025, modifié le 29/08/2025. Denière version : 15/09/2025.\n",
    "\n",
    "## Choix\n",
    "* On ne retient que la première affiliation de chaque auteur (pas les niveaux supérieurs : exemple : Boston University School of Medicine et pas Boston University).\n",
    "* Si un auteur rattaché à une structure française est aussi rattaché à une structure étrangère, on ne retient pas cet auteur pour compter une copublication internationale.\n",
    "* Si un auteur de la structure Inria recherchée est aussi affilié à une autre strucutre étrangère, celle-ci n'est pas mentionnée.\n",
    "\n",
    "\n",
    "## Étapes\n",
    "* Extraire les publications des équipes concernées.\n",
    "* identifier les publications dont les auteurs sont affiliés à un organisme étranger (hors France et DOM TOM)\n",
    "* On crée des listes d'identifiants uniques pour les affiliations FR, Union Européenne et hors UE.\n",
    "    *  on exclut les organismes étrangers dont les auteurs sont aussi affiliés à une structure FR\n",
    "    *  on exclut les affiliations en double pour une même publication\n",
    "    *  nettoyage des données.\n",
    "* Génération d'un fichier Excel avec : chiffres, liste des publications, liste des organismes étrangers copubliants\n",
    "* Première identification de la ville d'après \n",
    "    * dictionnaire déjà constitué par les recherches précédentes\n",
    "    * nom de la ville entre crochets dans le nom de l'organisme\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0c08c50",
   "metadata": {},
   "source": [
    "## Extraction des publications de HAL\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7973efcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "######## NOUVELLE VERSION ################\n",
    "import requests\n",
    "import time\n",
    "import os\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import logging\n",
    "from lxml import etree\n",
    "import locale\n",
    "\n",
    "# Dictionnaire des structures à interroger\n",
    "structures = {\n",
    "    \"419153\": \"Rennes\",\n",
    "    \"104751\": \"Bordeaux\",\n",
    "    \"34586\": \"Sophia\",\n",
    "    \"2497\": \"Grenoble\",\n",
    "    \"1096051\": \"Lyon\",\n",
    "    \"129671\": \"Nancy\",\n",
    "    \"104752\": \"Lille\",\n",
    "    \"118511\": \"Saclay\",\n",
    "    \"454310\": \"Paris\",\n",
    "    \"1175218\": \"Paris(sorb)\",\n",
    "    \"1225635\": \"Saclay (ipp)\",\n",
    "    \"1225627\": \"Saclay (UPS)\"\n",
    "}\n",
    "\n",
    "nom_collection = \"INRIA\"\n",
    "annee_debut = 2018\n",
    "annee_fin = 2025\n",
    "pas = 3\n",
    "\n",
    "# Codes pays France et DOM-TOM pour filtrage\n",
    "France_et_dom_tom_codes = ['FR','GP', 'RE', 'MQ', 'GF', 'YT', 'PM', 'WF', 'TF', 'NC', 'PF']\n",
    "\n",
    "# Initialisation globale des cumuls des données\n",
    "all_dataex = {}\n",
    "all_datafr = {}\n",
    "all_datapubli = []\n",
    "\n",
    "# Configuration du logger (répertoire et fichier)\n",
    "date_extraction_current = datetime.now().strftime(\"%Y-%m-%d\")\n",
    "log_directory = '../log/'\n",
    "os.makedirs(log_directory, exist_ok=True)\n",
    "log_file = os.path.join(log_directory, date_extraction_current + '__international_publications_log.txt')\n",
    "logging.basicConfig(filename=log_file, level=logging.INFO, format='%(asctime)s - %(message)s')\n",
    "\n",
    "# Configuration locale française\n",
    "locale.setlocale(locale.LC_TIME, \"French_France.1252\")\n",
    "\n",
    "def fetch_with_retry(url, params=None, max_retries=3, delay=2):\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            response = requests.get(url, params=params, timeout=10)\n",
    "            if response.status_code == 200:\n",
    "                return response\n",
    "            print(f\"⚠️ Tentative {attempt + 1} échouée ({response.status_code}). Nouvelle tentative...\")\n",
    "        except requests.RequestException as e:\n",
    "            print(f\"⏳ Erreur réseau ({e}), tentative {attempt + 1}...\")\n",
    "        time.sleep(delay)\n",
    "    return None\n",
    "\n",
    "def extraire_publications(id_aurehal, nom_struct):\n",
    "    base_url = f\"https://api.archives-ouvertes.fr/search/{nom_collection}?\"\n",
    "    for start_year in range(annee_debut, annee_fin + 1, pas):\n",
    "        end_year = min(start_year + pas - 1, annee_fin)\n",
    "        periode = f\"[{start_year} TO {end_year}]\"\n",
    "        print(f\"▶️ {nom_struct} ({id_aurehal}) : Traitement de la période : {periode}\")\n",
    "\n",
    "        params = {\n",
    "            \"q\": f\"publicationDateY_i:{periode}\",\n",
    "            \"fq\": f\"structId_i:{id_aurehal}\",\n",
    "            \"wt\": \"xml-tei\",\n",
    "            \"rows\": 1,\n",
    "            \"sort\": \"docid asc\"\n",
    "        }\n",
    "\n",
    "        cursor_mark = \"*\"\n",
    "        previous_cursor_mark = None\n",
    "        compteur = 0\n",
    "        partenaire = 0\n",
    "\n",
    "        unique_org_ex = {}\n",
    "        unique_org_fr = {}\n",
    "        datapubli = []\n",
    "\n",
    "        namespaces = {\"tei\": \"http://www.tei-c.org/ns/1.0\"}\n",
    "\n",
    "        while cursor_mark != previous_cursor_mark:\n",
    "            params[\"cursorMark\"] = cursor_mark\n",
    "            compteur += 1\n",
    "            if compteur % 5000 == 0 or compteur == 1:\n",
    "                print(f\"Nombre de notices traitées : {compteur}\")\n",
    "                # Optionnel: exporter ou sauvegarder les données partiellement ici\n",
    "\n",
    "            response = fetch_with_retry(base_url, params)\n",
    "            if not response:\n",
    "                print(\"Échec de la récupération des données après plusieurs tentatives.\")\n",
    "                break\n",
    "\n",
    "            try:\n",
    "                tree = etree.fromstring(response.content)\n",
    "            except etree.XMLSyntaxError:\n",
    "                print(\"Erreur de syntaxe XML. Réponse non analysée.\")\n",
    "                continue\n",
    "\n",
    "            next_cursor_mark = tree.attrib.get(\"next\")\n",
    "            quantity_value = tree.find('.//tei:measure', namespaces=namespaces).attrib.get('quantity') if tree.find('.//tei:measure', namespaces=namespaces) is not None else \"0\"\n",
    "\n",
    "            if cursor_mark == \"*\":\n",
    "                print(f\"Nombre total de résultats pour cette période : {quantity_value}. Estimation durée: ~20 mn pour 4000 notices.\")\n",
    "\n",
    "            biblfull_elements = tree.findall('.//tei:biblFull', namespaces=namespaces)\n",
    "            if not biblfull_elements:\n",
    "                if cursor_mark == next_cursor_mark:\n",
    "                    print(\"Aucune notice trouvée - fin de la récupération.\")\n",
    "                    break\n",
    "                else:\n",
    "                    print(\"Problème dans la réponse API, veuillez relancer.\")\n",
    "                    break\n",
    "            biblfull = biblfull_elements[0]\n",
    "\n",
    "            # Récupération id HAL\n",
    "            halID = biblfull.xpath('.//tei:publicationStmt/tei:idno[@type=\"halId\"]/text()', namespaces=namespaces) or [\"pas de hal_ID\"]\n",
    "            halID_value = halID[0]\n",
    "\n",
    "            # Traitement des affiliations\n",
    "            orgs = tree.findall('.//tei:listOrg[@type=\"structures\"]/tei:org', namespaces=namespaces)\n",
    "            for org in orgs:\n",
    "                xml_id = org.xpath('@xml:id', namespaces=namespaces)\n",
    "                lenom = org.xpath('.//tei:orgName/text()', namespaces=namespaces)\n",
    "                lacronyme = org.xpath('.//tei:orgName[@type=\"acronym\"]/text()', namespaces=namespaces)\n",
    "                lepays = org.xpath('.//tei:country/@key', namespaces=namespaces)\n",
    "                ladresse = [addr.text for addr in org.xpath('.//tei:addrLine', namespaces=namespaces) if addr.text]\n",
    "                ladresse_value = \" \".join(ladresse)\n",
    "                lesrelations = org.xpath('.//tei:listRelation/tei:relation/@active', namespaces=namespaces)\n",
    "\n",
    "                lesrelations_cleaned = [relation.replace('#struct-', '') for relation in lesrelations]\n",
    "                xml_id_cleaned = xml_id[0].lstrip('struct-') \n",
    "\n",
    "                # Exclusion non appliquée (décommenter si besoin)\n",
    "                # if xml_id_cleaned in equipes_a_exclure:\n",
    "                #     continue\n",
    "\n",
    "                if lepays and lepays[0] not in France_et_dom_tom_codes:\n",
    "                    partenaire = 1\n",
    "                    unique_org_ex[xml_id[0]] = {\n",
    "                        \"Pays_ex\": lepays,\n",
    "                        \"OrganismeEx\": lenom[0] if lenom else '',\n",
    "                        \"ID_aurehal\": xml_id_cleaned,\n",
    "                        \"adresse\": ladresse_value,\n",
    "                        \"parents\": lesrelations_cleaned\n",
    "                    }\n",
    "                elif lepays and lepays[0] in France_et_dom_tom_codes:\n",
    "                    unique_org_fr[xml_id[0]] = {\n",
    "                        \"Pays_fr\": lepays,\n",
    "                        \"Organisme_fr\": lenom[0] if lenom else '',\n",
    "                        \"Acronyme_fr\": lacronyme[0] if lacronyme else 'na',\n",
    "                        \"ID_aurehal\": xml_id_cleaned,\n",
    "                        \"adresse\": ladresse_value,\n",
    "                        \"parents\": lesrelations_cleaned\n",
    "                    }\n",
    "\n",
    "            # Filtrer selon partenaire = 1 seulement, sinon prendre toutes les publications\n",
    "            if partenaire == 1: \n",
    "\n",
    "                date_value = biblfull.xpath('.//tei:sourceDesc/tei:biblStruct//tei:monogr/tei:imprint/tei:date[@type=\"datePub\"]/text()', namespaces=namespaces)\n",
    "                date_produced = biblfull.xpath('.//tei:editionStmt/tei:edition/tei:date[@type=\"whenProduced\"]/text()', namespaces=namespaces)\n",
    "                if date_value and date_value[0]:\n",
    "                    year_value = date_value[0][:4]\n",
    "                elif date_produced and date_produced[0]:\n",
    "                    year_value = date_produced[0][:4]\n",
    "                else:\n",
    "                    year_value = \"\"\n",
    "\n",
    "                keywords = biblfull.xpath('.//tei:profileDesc/tei:textClass/tei:keywords/tei:term', namespaces=namespaces)\n",
    "                keywords_str = \";\".join(\n",
    "                    \" \".join(term.text.split())\n",
    "                    for term in keywords if term.text\n",
    "                )\n",
    "\n",
    "                hal_domain_elems = biblfull.xpath('.//tei:profileDesc/tei:textClass/tei:classCode[@scheme=\"halDomain\"]', namespaces=namespaces)\n",
    "                hal_domain_str = \";\".join(elem.text.strip() for elem in hal_domain_elems if elem.text) if hal_domain_elems else \"\"\n",
    "\n",
    "                def get_full_text(elem):\n",
    "                    return \"\".join(elem.itertext()).strip() if elem is not None else \"\"\n",
    "\n",
    "                abstract_elem = biblfull.xpath('.//tei:profileDesc/tei:abstract[@xml:lang=\"en\"]', namespaces=namespaces)\n",
    "                if not abstract_elem:\n",
    "                    abstract_elem = biblfull.xpath('.//tei:profileDesc/tei:abstract[@xml:lang=\"fr\"]', namespaces=namespaces)\n",
    "                abstract_str = get_full_text(abstract_elem[0]) if abstract_elem else \"\"\n",
    "\n",
    "                # Extraction auteurs et affiliations\n",
    "                for author in biblfull.xpath('.//tei:titleStmt/tei:author', namespaces=namespaces):\n",
    "                    forename = author.xpath('.//tei:persName/tei:forename/text()', namespaces=namespaces) or [\"Unknown\"]\n",
    "                    surname = author.xpath('.//tei:persName/tei:surname/text()', namespaces=namespaces) or [\"Unknown\"]\n",
    "                    authorLastFirstnames = f\"{surname[0]}, {forename[0]}\"\n",
    "\n",
    "                    affiliations = author.xpath('.//tei:affiliation/@ref', namespaces=namespaces)\n",
    "                    for affiliation in affiliations:\n",
    "                        affiliation = affiliation.lstrip('#struct-')\n",
    "                        # if affiliation in equipes_a_exclure:\n",
    "                        #     continue\n",
    "\n",
    "                        datapubli.append({\n",
    "                            \"halID\": halID_value,\n",
    "                            \"Auteur\": authorLastFirstnames,\n",
    "                            \"affiliation\": affiliation,\n",
    "                            \"Centre\": nom_struct,\n",
    "                            \"Annee\": year_value,\n",
    "                            \"MotsCles\": keywords_str,\n",
    "                            \"Domaine(s)\": hal_domain_str,\n",
    "                            \"Resume\": abstract_str,\n",
    "                        })\n",
    "                partenaire = 0\n",
    "\n",
    "            previous_cursor_mark = cursor_mark\n",
    "            cursor_mark = next_cursor_mark\n",
    "\n",
    "            if not next_cursor_mark:\n",
    "                print(f\"Fin des résultats pour {nom_struct} période {periode}, total notices traitées : {compteur}\")\n",
    "                break\n",
    "            time.sleep(0.1)  # Pause courte pour ne pas surcharger\n",
    "\n",
    "        all_dataex.update(unique_org_ex)\n",
    "        all_datafr.update(unique_org_fr)\n",
    "        all_datapubli.extend(datapubli)\n",
    "\n",
    "# Boucle principale sur chaque structure au dictionnaire\n",
    "for id_aurehal, nom_struct in structures.items():\n",
    "    extraire_publications(id_aurehal, nom_struct)\n",
    "\n",
    "print(\"Extraction terminée. Résultats cumulés dans all_dataex, all_datafr, all_datapubli\")\n",
    "\n",
    "#Temps de traitement : entre 4 et 5h."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c64bfad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################################################\n",
    "# Conversion en \"dataframes\" pour traitement des données et comptage\n",
    "######################################################################\n",
    "\n",
    "df_ex = pd.DataFrame(list(all_dataex.values()))\n",
    "df_fr = pd.DataFrame(list(all_datafr.values()))\n",
    "df_publis = pd.DataFrame(all_datapubli)\n",
    "\n",
    "# Sauvegarde pour contrôle et tests\n",
    "# df_ex.to_excel(\"df_ex_total.xlsx\", index=False)\n",
    "# df_fr.to_excel(\"df_fr_total.xlsx\", index=False)\n",
    "# df_publis.to_excel(\"df_publis_total.xlsx\", index=False)\n",
    "print(\"dataframes créés\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "450234ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sauvegarde pour contrôle et tests\n",
    "df_ex.to_excel(\"df_ex_total.xlsx\", index=False)\n",
    "df_fr.to_excel(\"df_fr_total.xlsx\", index=False)\n",
    "df_publis.to_excel(\"df_publis_total.xlsx\", index=False)\n",
    "print(\"dataframes créés\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d79ae6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  pour tests : df_publis=pd.read_excel(\"df_publis_total.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d67dd20d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_publis.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b5a75aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "####################################\n",
    "# FILTRE UE / hors UE pour df_ex\n",
    "####################################\n",
    "df_nonUE = \"\"\n",
    "df_UE = \"\"\n",
    "pays_UE = [\n",
    "    \"AT\", \"BE\", \"BG\", \"HR\", \"CY\", \"CZ\", \"DK\", \"EE\", \"FI\", \"FR\",\n",
    "    \"DE\", \"GR\", \"HU\", \"IE\", \"IT\", \"LV\", \"LT\", \"LU\", \"MT\", \"NL\",\n",
    "    \"PL\", \"PT\", \"RO\", \"SK\", \"SI\", \"ES\", \"SE\"\n",
    "]\n",
    "\n",
    "# On va créer 2 dataframes en fonction du pays de la structure (UE ou hors UE)\n",
    "#\n",
    "\n",
    "# Fonction pour vérifier si une valeur de la liste est dans la colonne `identifiant` ou `parents`\n",
    "def filter_rows_EU(row):\n",
    "    # Vérifie si une des valeurs de la liste se trouve dans `country` ou dans les valeurs de `parents`\n",
    "    return any(country in pays_UE for country in row[\"Pays_ex\"])\n",
    "\n",
    "def filter_rows_ex(row):\n",
    "    # Vérifie si une des valeurs de la liste ne se trouve pas dans `country` ou dans les valeurs de `parents`\n",
    "    return any(country not in pays_UE for country in row[\"Pays_ex\"])\n",
    "\n",
    "\n",
    "# Filtrer les lignes du DataFrame pour garder celles des structures qui nous intéressent\n",
    "df_UE = df_ex[df_ex.apply(filter_rows_EU, axis=1)]\n",
    "df_UE.rename(columns={\"OrganismeEx\" : \"Organisme_UE\"}, inplace=True)\n",
    "\n",
    "\n",
    "df_nonUE = df_ex[df_ex.apply(filter_rows_ex, axis=1)]\n",
    "df_nonUE.rename(columns={\"OrganismeEx\" : \"Organisme_Hors_UE\"}, inplace=True)\n",
    "\n",
    "\n",
    "print(\"Dataframes UE et non UE créés\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66a09ef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################################################\n",
    "# Interprétation des codes Pays en noms en toutes lettres \n",
    "###########################################################\n",
    "\n",
    "# Récupérer les données de l'API\n",
    "# url = \"https://restcountries.com/v3.1/all\"\n",
    "# response = requests.get(url)\n",
    "# countries_data = response.json()\n",
    "import requests\n",
    "\n",
    "def get_country_mapping():\n",
    "    url = \"https://restcountries.com/v3.1/all\"\n",
    "    params = {\"fields\": \"cca2,name\"}\n",
    "    \n",
    "    try:\n",
    "        response = requests.get(url, params=params, timeout=10)\n",
    "        if response.status_code == 200:\n",
    "            countries_data = response.json()\n",
    "            if isinstance(countries_data, list):\n",
    "                return {\n",
    "                    country.get(\"cca2\"): country.get(\"name\", {}).get(\"common\")\n",
    "                    for country in countries_data\n",
    "                    if country.get(\"cca2\") and \"name\" in country and \"common\" in country[\"name\"]\n",
    "                }\n",
    "            else:\n",
    "                print(\"⚠️ Format inattendu :\", type(countries_data))\n",
    "                return {}\n",
    "        else:\n",
    "            print(f\"⚠️ Erreur API ({response.status_code}): {response.json().get('message')}\")\n",
    "            return {}\n",
    "    except Exception as e:\n",
    "        print(\"❌ Erreur lors de la récupération des données pays :\", e)\n",
    "        return {}\n",
    "\n",
    "# Utilisation\n",
    "country_mapping = get_country_mapping()\n",
    "print(\"✅ Exemple : FR →\", country_mapping.get(\"FR\"))  # Affiche 'France'\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "df_UE[\"Pays_ex\"] = df_UE[\"Pays_ex\"].apply(lambda x: country_mapping.get(x[0]) if isinstance(x, list) and x else x)\n",
    "df_UE['TypePays'] = \"EU\"\n",
    "\n",
    "df_nonUE[\"Pays_ex\"] = df_nonUE[\"Pays_ex\"].apply(lambda x: country_mapping.get(x[0]) if isinstance(x, list) and x else x)\n",
    "df_nonUE['TypePays'] = \"EX\"\n",
    "\n",
    "df_fr[\"Pays_fr\"] = df_fr[\"Pays_fr\"].apply(lambda x: country_mapping.get(x[0]) if isinstance(x, list) and x else x)\n",
    "df_fr['TypePays'] = \"FR\"\n",
    "\n",
    "\n",
    "# Afficher un aperçu du DataFrame modifié\n",
    "print(df_UE.head(1))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "212e9571",
   "metadata": {},
   "outputs": [],
   "source": [
    "########## \n",
    "# Ajouter l'acronyme des auteurs Inria dans la liste des publications\n",
    "###########\n",
    "\n",
    "# Charger ton fichier d'équipes Inria\n",
    "df_equipes = pd.read_excel(\"equipesInriadeAurehal.xlsx\")\n",
    "\n",
    "# Vérifie que les colonnes utiles existent\n",
    "assert \"docid\" in df_equipes.columns, \"Colonne 'docid' manquante dans le fichier Excel\"\n",
    "assert \"acronyme\" in df_equipes.columns, \"Colonne 'acronyme' manquante dans le fichier Excel\"\n",
    "\n",
    "# S'assurer que 'docid' et 'affiliation' sont bien de type str\n",
    "df_equipes[\"docid\"] = df_equipes[\"docid\"].astype(str)\n",
    "df_publis[\"affiliation\"] = df_publis[\"affiliation\"].astype(str)\n",
    "\n",
    "# Fusion des deux DataFrames : on ajoute l'acronyme en fonction de l'affiliation\n",
    "df_publis = df_publis.merge(df_equipes[[\"docid\", \"acronyme\"]], how=\"left\", left_on=\"affiliation\", right_on=\"docid\")\n",
    "\n",
    "# Optionnel : supprimer la colonne docid (redondante après le merge)\n",
    "df_publis.drop(columns=[\"docid\"], inplace=True)\n",
    "\n",
    "# Exemple d'affichage\n",
    "print(df_publis[[\"halID\", \"Auteur\", \"affiliation\", \"acronyme\"]].head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55be05e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  pour tests \n",
    "df_publis[df_publis[\"halID\"] == \"hal-00799242\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd4e5c24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nettoyage : Pour les auteurs Inria (auteurs ayant un acronyme), supprimer les autres affiliations\n",
    "# Marquer les cas où pour chaque (halID, Auteur), au moins un acronyme est non-null\n",
    "df=df_publis\n",
    "\n",
    "df[\"has_acronyme\"] = df.groupby([\"halID\", \"Auteur\"])[\"acronyme\"].transform(lambda x: x.notna().any())\n",
    "\n",
    "# Ne garder que :\n",
    "# - les lignes où acronyme est non-null\n",
    "# - ou les cas où aucune ligne pour ce (halID, Auteur) n'a d'acronyme\n",
    "df = df[(df[\"acronyme\"].notna()) | (~df[\"has_acronyme\"])]\n",
    "\n",
    "# Supprimer la colonne temporaire\n",
    "df = df.drop(columns=[\"has_acronyme\"])\n",
    "\n",
    "df_publis = df\n",
    "df_publis.head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ab58e93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ajouter l'organisme et le pays UE des auteurs à la liste générale des publications\n",
    "\n",
    "# Vérifier que les colonnes existent\n",
    "assert \"ID_aurehal\" in df_UE.columns, \"Colonne 'ID_aurehal' manquante dans df_UE\"\n",
    "assert \"Organisme_UE\" in df_UE.columns, \"Colonne 'Organisme_UE' manquante dans df_UE\"\n",
    "assert \"Pays_ex\" in df_UE.columns, \"Colonne 'Pays_ex' manquante dans df_UE\"\n",
    "assert \"adresse\" in df_UE.columns, \"Colonne 'adresse' manquante dans df_UE\"\n",
    "\n",
    "# Harmoniser les types de colonnes pour le merge\n",
    "df_UE[\"ID_aurehal\"] = df_UE[\"ID_aurehal\"].astype(str)\n",
    "df_publis[\"affiliation\"] = df_publis[\"affiliation\"].astype(str)\n",
    "\n",
    "# Renommer les colonnes de df_nonUE pour éviter les collisions\n",
    "df_UE_renamed = df_UE.rename(columns={\n",
    "    \"adresse\": \"adresse_UE\",  # Pour éviter d’écraser la précédente\n",
    "})\n",
    "\n",
    "# Faire la jointure\n",
    "df_publis = df_publis.merge(\n",
    "    df_UE_renamed[[\"ID_aurehal\", \"Organisme_UE\", \"Pays_ex\",\"adresse_UE\"]],\n",
    "    how=\"left\",\n",
    "    left_on=\"affiliation\",\n",
    "    right_on=\"ID_aurehal\"\n",
    ")\n",
    "\n",
    "# Supprimer la colonne intermédiaire redondante\n",
    "df_publis.drop(columns=[\"ID_aurehal\"], inplace=True)\n",
    "\n",
    "# Vérification du résultat\n",
    "print(df_publis[[\"affiliation\", \"Organisme_UE\", \"Pays_ex\",\"adresse_UE\"]].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "962cb9f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_nonUE.head(1) #pour connaître le nom des colonnes afin de faire le traitement suivant\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad7fc718",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ajouter l'organisme et le pays Hors UE des auteurs\n",
    "\n",
    "# Vérifier que les colonnes existent\n",
    "assert \"ID_aurehal\" in df_nonUE.columns, \"Colonne 'ID_aurehal' manquante dans df_nonUE\"\n",
    "assert \"Organisme_Hors_UE\" in df_nonUE.columns, \"Colonne 'Organisme_Hors_UE' manquante dans df_nonUE\"\n",
    "assert \"Pays_ex\" in df_nonUE.columns, \"Colonne 'Pays_ex' manquante dans df_nonUE\"\n",
    "assert \"adresse\" in df_nonUE.columns, \"Colonne 'adresse' manquante dans df_nonUE\"\n",
    "\n",
    "# Harmoniser les types de colonnes pour le merge\n",
    "df_nonUE[\"ID_aurehal\"] = df_nonUE[\"ID_aurehal\"].astype(str)\n",
    "df_publis[\"affiliation\"] = df_publis[\"affiliation\"].astype(str)\n",
    "\n",
    "# Renommer les colonnes de df_nonUE pour éviter les collisions\n",
    "df_nonUE_renamed = df_nonUE.rename(columns={\n",
    "    \"Pays_ex\": \"Pays_ex_horsUE\",  # Pour éviter d’écraser la précédente\n",
    "    \"adresse\": \"adresse_hors_UE\", \n",
    "})\n",
    "\n",
    "# Faire la jointure\n",
    "df_publis = df_publis.merge(\n",
    "    df_nonUE_renamed[[\"ID_aurehal\", \"Organisme_Hors_UE\", \"Pays_ex_horsUE\",\"adresse_hors_UE\"]],\n",
    "    how=\"left\",\n",
    "    left_on=\"affiliation\",\n",
    "    right_on=\"ID_aurehal\"\n",
    ")\n",
    "\n",
    "# Supprimer la colonne intermédiaire redondante\n",
    "df_publis.drop(columns=[\"ID_aurehal\"], inplace=True)\n",
    "\n",
    "# Vérification du résultat\n",
    "print(df_publis[[\"affiliation\", \"Organisme_Hors_UE\", \"Pays_ex_horsUE\",\"adresse_hors_UE\"]].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51d4b603",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  contrôle pour tests \n",
    "df_publis[df_publis[\"halID\"] == \"hal-00799242\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b0381c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "##########################################\n",
    "# Supprimer les auteurs HORS INRIA qui ont à la fois une affiliation française et une affiliation étrangère)\n",
    "#########################################\n",
    "# Indicateurs\n",
    "df_publis[\"has_org\"] = df_publis[\"Organisme_UE\"].notna() | df_publis[\"Organisme_Hors_UE\"].notna()\n",
    "df_publis[\"has_no_acronyme\"] = df_publis[\"acronyme\"].isna() | (df_publis[\"acronyme\"].str.strip() == \"\")\n",
    "\n",
    "# Grouper par halID + Auteur\n",
    "grouped = df_publis.groupby([\"halID\", \"Auteur\"]).agg(\n",
    "    n_lignes=(\"halID\", \"count\"),          # nombre de lignes pour ce couple\n",
    "    has_no_acronyme=(\"has_no_acronyme\", \"any\"),\n",
    "    has_org=(\"has_org\", \"any\")\n",
    ").reset_index()\n",
    "\n",
    "# On garde uniquement ceux qui ont au moins 2 lignes et les deux cas\n",
    "auteurs_mixtes = grouped[\n",
    "    (grouped[\"n_lignes\"] >= 2) &\n",
    "    (grouped[\"has_no_acronyme\"]) &\n",
    "    (grouped[\"has_org\"])\n",
    "]\n",
    "\n",
    "# Supprimer ces auteurs de df_publis\n",
    "df_publis_clean = df_publis.merge(\n",
    "    auteurs_mixtes[[\"halID\", \"Auteur\"]],\n",
    "    on=[\"halID\", \"Auteur\"],\n",
    "    how=\"left\",\n",
    "    indicator=True\n",
    ")\n",
    "df_publis_clean = df_publis_clean[df_publis_clean[\"_merge\"] == \"left_only\"].drop(columns=\"_merge\")\n",
    "\n",
    "print(f\"✅ {len(df_publis) - len(df_publis_clean)} lignes supprimées\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3fb0b9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# contrôle pour test \n",
    "df_publis_clean[df_publis_clean[\"halID\"] == \"hal-00799242\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdf9e528",
   "metadata": {},
   "outputs": [],
   "source": [
    "# contrôle pour test \n",
    "df_publis_clean[df_publis_clean[\"halID\"] == \"hal-01895279\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cefbfa14",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_publis = df_publis_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd359ea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nettoyage : On supprime tous les co-auteurs français = ne sont pas Inria (pas d'acronyme) et n'ont pas d'affiliations étrangères\n",
    "\n",
    "# Colonnes à tester pour le vide\n",
    "cols_to_check = [\"Organisme_UE\", \"Organisme_Hors_UE\", \"acronyme\"]\n",
    "\n",
    "# Masque des lignes vides\n",
    "mask_empty = df_publis[cols_to_check].apply(\n",
    "    lambda row: all(pd.isna(v) or str(v).strip() == \"\" for v in row),\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# On garde seulement les lignes qui ne sont pas \"vides\"\n",
    "df_publis_clean = df_publis[~mask_empty].copy()\n",
    "\n",
    "print(f\"✅ {mask_empty.sum()} lignes supprimées\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa0baa8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# contrôle pour test \n",
    "df_publis_clean[df_publis_clean[\"halID\"] == \"hal-01895279\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4909a133",
   "metadata": {},
   "outputs": [],
   "source": [
    "# contrôle pour test df_publis_clean[df_publis_clean[\"halID\"] == \"cea-04228169\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deedb92d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_publis = df_publis_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cef4a46",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_publis.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c48a135",
   "metadata": {},
   "outputs": [],
   "source": [
    "# On ne garde pas les publications avec juste un seul auteur\n",
    "df_publis_clean = df_publis[df_publis.groupby(\"halID\")[\"halID\"].transform(\"count\") > 1].copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a4fbaf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# contrôle pour test \n",
    "df_publis_clean[df_publis_clean[\"halID\"] == \"hal-01895279\"] # ne doit pas y être"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e05e129d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# contrôle pour test df_publis_clean[df_publis_clean[\"halID\"] == \"hal-05212970\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5afecece",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_publis = df_publis_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be3c16ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_publis.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be216cfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fichier final (1e partie) avec , pour chaque auteur Inria, les copubliants étrangers\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# 1. On part du df_publis et on isole les auteurs FR et étrangers\n",
    "auteurs_fr = df_publis[pd.notna(df_publis['acronyme']) & (df_publis['acronyme'].str.strip() != '')]\n",
    "auteurs_etr = df_publis[pd.isna(df_publis['acronyme']) | (df_publis['acronyme'].str.strip() == '')]\n",
    "\n",
    "# 2. Pour chaque auteur étranger, on veut rattacher les auteurs FR du même halID\n",
    "rows = []\n",
    "for _, row_etr in auteurs_etr.iterrows():\n",
    "    hal_id = row_etr['halID']\n",
    "    # Trouver les auteurs FR liés à ce halID\n",
    "    fr_list = auteurs_fr[auteurs_fr['halID'] == hal_id]\n",
    "    for _, row_fr in fr_list.iterrows():\n",
    "        rows.append({\n",
    "            'Equipe': row_fr['acronyme'],\n",
    "            'Centre':row_fr['Centre'],\n",
    "            'Auteurs FR': row_fr['Auteur'],\n",
    "            'Auteurs copubliants': row_etr['Auteur'],\n",
    "            'Organisme copubliant': row_etr['Organisme_Hors_UE'] if pd.notna(row_etr['Organisme_Hors_UE']) else row_etr['Organisme_UE'],\n",
    "            'Adresse': row_etr['adresse_hors_UE'] if pd.notna(row_etr['adresse_hors_UE']) else row_etr['adresse_UE'],\n",
    "            'Pays': row_etr['Pays_ex_horsUE'] if pd.notna(row_etr['Pays_ex_horsUE']) else row_etr['Pays_ex'],\n",
    "            'ID Aurehal': row_etr['affiliation'],\n",
    "            'Année': row_etr['Annee'],\n",
    "            'UE/Non UE': 'UE' if pd.notna(row_etr['Organisme_UE']) else 'Non UE',\n",
    "            'HalID': hal_id,\n",
    "            'Domaine(s)': row_etr['Domaine(s)'],\n",
    "            'Mots-cles' : row_etr['MotsCles'],\n",
    "            'Resume':row_etr['Resume'],\n",
    "        })\n",
    "\n",
    "# 3. Construire le DataFrame final\n",
    "df_final = pd.DataFrame(rows)\n",
    "\n",
    "\n",
    "# 4. Trier par Equipe, Auteurs FR, Auteurs copubliants\n",
    "df_final = df_final.sort_values(by=['Equipe', 'Auteurs FR', 'Auteurs copubliants']).reset_index(drop=True)\n",
    "\n",
    "\n",
    "# 5. Exporter vers Excel\n",
    "nom_du_fichier = f\"Copubliants_par_auteur_Inria_tout.xlsx\"\n",
    "df_final.to_excel(nom_du_fichier, index=False)\n",
    "\n",
    "print(f\"✅ Fichier Excel créé : {nom_du_fichier}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cb1a052",
   "metadata": {},
   "outputs": [],
   "source": [
    "# contrôle pour test df_final[df_final[\"HalID\"] == \"hal-00799242\"]\n",
    "df_final.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e0e3360",
   "metadata": {},
   "outputs": [],
   "source": [
    "# décommenter la ligne suivante ligne si la librairie geotext n'est pas installée \n",
    "# pip install geotext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc502f95",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(nom_du_fichier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea289437",
   "metadata": {},
   "outputs": [],
   "source": [
    "###################\n",
    "# Premier repérage des villes :\n",
    "#1 Utilisation du fichier contenant la liste déjà vérifiée des villes associées à un ID Aurehal = dictionnaire des villes aurehal\n",
    "\n",
    "#####################\"\"\n",
    "\n",
    "import pandas as pd\n",
    "from geotext import GeoText\n",
    "import re\n",
    "\n",
    "df_publis_tout = \"\"\n",
    "###########################################\n",
    "# Chargement du fichier des copublications\n",
    "##########################################\n",
    "df_publis_tout = pd.read_excel(nom_du_fichier)\n",
    "\n",
    "\n",
    "#######################\n",
    "# Renseigner avec le dictionnaire déjà existant\n",
    "######################\n",
    "df_villes =\"\"\n",
    "# Chargement du fichier ID Aurehal - Ville\n",
    "df_villes = pd.read_excel(\"ID_Aurehal_Ville_Etat_Latitude_Longitude.xlsx\")\n",
    "\n",
    "\n",
    "# Fusionner les deux DataFrames sur la colonne ID_Aurehal\n",
    "df_publis_tout = df_publis_tout.merge(\n",
    "    df_villes[[\"ID_Aurehal\",\"Ville\",\"StateCode\", \"Latitude\", \"Longitude\",\"geonameid\"]],\n",
    "    left_on=\"ID Aurehal\", #nom de la colonne dans df_publis_tout\n",
    "    right_on=\"ID_Aurehal\",#nom de la colonne dans df_villes\n",
    "    how=\"left\"  # garde toutes les lignes de df_publis_tout, même si pas de correspondance\n",
    ")\n",
    "\n",
    "# Suppression de la colonne ID_Aurehal, qui ne nous sert plus\n",
    "df_publis_tout = df_publis_tout.drop(columns=[\"ID_Aurehal\"])\n",
    "\n",
    "df_publis_tout.to_excel(\"resultat_avec_villes_du_dictionnaire.xlsx\", index=False)\n",
    "\n",
    "# temps de traitement normal environ 20 secondes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98eb5ede",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pour contrôle visuel, il doit y avoir un résultat pour cette référence\n",
    "df_publis_tout[df_publis_tout[\"HalID\"] == \"hal-05212970\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dce27c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#################################################################################\n",
    "# 2e étape Identification des villes entre crochets dans le nom de l'organisme (il manquera encore la latitude et la longitude)\n",
    "################################################################################\n",
    "\n",
    "# Liste des pays à ignorer\n",
    "pays_a_ignorer = {\n",
    "    \"Algeria\", \"Argentina\", \"Australia\", \"Austria\", \"Belgium\", \"Bolivia\",\n",
    "    \"Bosnia and Herzegovina\", \"Brazil\", \"Brunei\", \"Bulgaria\", \"Burkina Faso\",\n",
    "    \"Cameroon\", \"Canada\", \"Chile\", \"China\", \"Colombia\", \"Costa Rica\", \"Croatia\",\n",
    "    \"Cyprus\", \"Czechia\", \"Denmark\", \"Ecuador\", \"Estonia\", \"Finland\", \"Georgia\",\n",
    "    \"Germany\", \"Greece\", \"Hong Kong\", \"Hungary\", \"Iceland\", \"India\", \"Indonesia\",\n",
    "    \"Iran\", \"Ireland\", \"Israel\", \"Italy\", \"Japan\", \"Jordan\", \"Kenya\", \"Latvia\",\n",
    "    \"Lebanon\", \"Lithuania\", \"Luxembourg\", \"Madagascar\", \"Malaysia\", \"Malta\",\n",
    "    \"Mexico\", \"Morocco\", \"Netherlands\", \"New Zealand\", \"Niger\", \"Nigeria\",\n",
    "    \"North Macedonia\", \"Norway\", \"Oman\", \"Pakistan\", \"Peru\", \"Poland\", \"Portugal\",\n",
    "    \"Romania\", \"Russia\", \"Saudi Arabia\", \"Senegal\", \"Serbia\", \"Singapore\",\n",
    "    \"Slovakia\", \"Slovenia\", \"South Africa\", \"South Korea\", \"Spain\", \"Sweden\",\n",
    "    \"Switzerland\", \"Taiwan\", \"Thailand\", \"Tunisia\", \"Turkey\", \"Uganda\", \"Ukraine\",\n",
    "    \"United Arab Emirates\", \"United Kingdom\", \"United States\", \"Uruguay\",\n",
    "    \"Venezuela\", \"Vietnam\"\n",
    "}\n",
    "\n",
    "# Fonction pour extraire la ville entre crochets\n",
    "def get_ville(organisme, adresse):\n",
    "    if isinstance(organisme, str):\n",
    "        match = re.search(r\"\\[(.*?)\\]\", organisme)\n",
    "        if match:\n",
    "            contenu = match.group(1).strip()\n",
    "            if contenu not in pays_a_ignorer:\n",
    "                return contenu\n",
    "    return None\n",
    "\n",
    "# Appliquer la fonction UNIQUEMENT si Ville est vide\n",
    "mask = df_publis_tout[\"Ville\"].isna() | (df_publis_tout[\"Ville\"] == \"\")\n",
    "df_publis_tout.loc[mask, \"Ville\"] = df_publis_tout[mask].apply(\n",
    "    lambda row: get_ville(row[\"Organisme copubliant\"], row[\"Adresse\"]), axis=1\n",
    ")\n",
    "\n",
    "# Créer un DataFrame avec les lignes où une ville a été extraite\n",
    "df_matches = df_publis_tout[df_publis_tout[\"Ville\"].notna() &\n",
    "                            (df_publis_tout[\"Ville\"] != \"\")].copy()\n",
    "\n",
    "# Garder uniquement les colonnes souhaitées et supprimer les doublons sur ID Aurehal\n",
    "df_matches = df_matches[[\"ID Aurehal\", \"Organisme copubliant\", \"Ville\"]].drop_duplicates(subset=[\"ID Aurehal\"])\n",
    "\n",
    "# Exporter la liste des ID avec Organisme copubliant et Ville\n",
    "# on ajoutera les latitudes et longitudes déjà connues\n",
    "# ensuite on vérifiera les inconnues pour voir si la ville est correctement identifiée\n",
    "df_matches.to_excel(\"id_aurehal_avec_ville_extraite.xlsx\", index=False)\n",
    "\n",
    "# Réordonner les colonnes si besoin\n",
    "cols_order = [\n",
    "    'Centre', 'Equipe', 'Auteurs FR', 'Auteurs copubliants', 'Organisme copubliant',\n",
    "    'Adresse', 'Ville', 'Pays', 'ID Aurehal', 'UE/Non UE', 'Année',\n",
    "    'HalID', 'Domaine(s)', 'Mots-cles', 'Resume',\"Latitude\", \"Longitude\",\"geonameid\"\n",
    "]\n",
    "if all(col in df_publis_tout.columns for col in cols_order):\n",
    "    df_publis_tout = df_publis_tout[cols_order]\n",
    "\n",
    "\n",
    "# Exporter le résultat principal\n",
    "df_publis_tout.to_excel(\"resultat_avec_villes_completes_dico_et_crochets.xlsx\", index=False)\n",
    "\n",
    "# Temps de traitement, environ 12 secondes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9980b4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# On renseigne la latitude et la longitude des villes déjà connues dans le dictionnaire de référence, parmi celles qui ont été trouvées entre crochets\n",
    "\n",
    "# On ne garde que le premier mot avant la virgule (ex: Richmond, UK --> Richmond)\n",
    "df_publis_tout[\"Ville\"] = df_publis_tout[\"Ville\"].str.split(\",\").str[0].str.strip()\n",
    "\n",
    "\n",
    "# Sélectionner les colonnes utiles dans df_villes\n",
    "df_villes_geo = \"\"\n",
    "df_publis_tout_geo = df_publis_tout\n",
    "\n",
    "df_villes_geo = df_villes[[\"Ville\", \"Pays\", \"Latitude\", \"Longitude\", \"geonameid\"]].drop_duplicates()\n",
    "\n",
    "# Fusionner avec df_publis_tout sur Ville et Pays\n",
    "df_temp = df_publis_tout_geo.merge(\n",
    "    df_villes_geo,\n",
    "    on=[\"Ville\", \"Pays\"],\n",
    "    how=\"left\",\n",
    "    suffixes=(\"_old\", \"_new\")\n",
    ")\n",
    "\n",
    "\n",
    "# Mettre à jour Latitude/Longitude uniquement si elles sont vides\n",
    "mask_lat = df_publis_tout_geo[\"Latitude\"].isna() & df_temp[\"Latitude_new\"].notna()\n",
    "mask_lon = df_publis_tout_geo[\"Longitude\"].isna() & df_temp[\"Longitude_new\"].notna()\n",
    "\n",
    "df_publis_tout_geo.loc[mask_lat, \"Latitude\"] = df_temp.loc[mask_lat, \"Latitude_new\"]\n",
    "df_publis_tout_geo.loc[mask_lon, \"Longitude\"] = df_temp.loc[mask_lon, \"Longitude_new\"]\n",
    "\n",
    "# mettre à jour geonameid si nécessaire\n",
    "if \"geonameid\" in df_publis_tout_geo.columns:\n",
    "    mask_geo = df_publis_tout_geo[\"geonameid\"].isna() & df_temp[\"geonameid_new\"].notna()\n",
    "    df_publis_tout_geo.loc[mask_geo, \"geonameid\"] = df_temp.loc[mask_geo, \"geonameid_new\"]\n",
    "\n",
    "\n",
    "# Exporter le résultat pour vérifier\n",
    "df_publis_tout_geo.to_excel(\"Copublis_Inria_villes_a_completer.xlsx\", index=False)\n",
    "\n",
    "# Temps de traitement environ 15 secondes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81713d89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# inspection d'une référence pour contrôle visuel du résultat\n",
    "df_publis_tout_geo[df_publis_tout_geo[\"HalID\"] == \"hal-03696264\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae7e6126",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Créer un fichier à part pour toutes les villes non trouvées\n",
    "\n",
    "df_villes_vides = \"\"\n",
    "# 1. Filtrer les lignes où Ville est vide\n",
    "df_villes_vides = df_publis_tout_geo[df_publis_tout_geo[\"Ville\"].isna()]\n",
    "\n",
    "# 2. Grouper et agréger\n",
    "df_villes_vides_uniques2 = df_villes_vides.groupby(\"ID Aurehal\").agg({\n",
    "    \"Organisme copubliant\": \"first\",\n",
    "    \"Adresse\": \"first\",\n",
    "    \"Pays\": \"first\",\n",
    "    \"HalID\": lambda x: \", \".join(x.unique())\n",
    "}).reset_index()\n",
    "\n",
    "# 3. Exporter\n",
    "df_villes_vides_uniques2.to_excel(\"villes_vides_uniques_aprs_dico_crochet_et_dico.xlsx\", index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a930e774",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Contrôle visuel du résultat\n",
    "df.head(3)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
